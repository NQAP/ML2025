# -*- coding: utf-8 -*-
import os
import random
import csv
import torch
import torch.nn as nn
import numpy as np
import pandas as pd

import torchvision.transforms as T
import matplotlib.pyplot as plt

from torch.utils.data import Dataset, DataLoader
from PIL import Image
from argparse import Namespace
from tqdm import tqdm

"""2025ML_HW2_sample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YT4zCSCfrPnry01kM-xI2brJYQUuE6rz

# ML HW2 sample code
TODO:
 - Design your CNN model (you are welcomed to use existing models)
 - Hyperparameter tuning
 - Confusion matrix

Report:
 - Structure of CNN model
 - Data Augmentation
 - Draw the confusion matrix

#### Download data
"""

# !gdown 19oEShkdcBJf41nu46anyvu7BgXUffIGi
# !unzip -q 'HW2.zip'

"""#### Import packages"""


"""#### Hyperparameters and setting"""

# TODO: modify the hyperparameters
config = Namespace(
    random_seed = 42,
    BATCH = 128,
    n_epoch = 30,
    lr = 1.5e-3,
    weight_decay = 1e-5,
    ckpt_path = 'model.pth',
)

TRA_PATH = 'data/train/'
TST_PATH = 'data/test/'
LABEL_PATH = 'data/train.csv'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.manual_seed(config.random_seed)
torch.cuda.manual_seed_all(config.random_seed)
random.seed(config.random_seed)
np.random.seed(config.random_seed)

"""#### Dataset and Dataloader"""

class FaceExpressionDataset(Dataset):
    def __init__(self, img_path, label_path=None, tfm=T.ToTensor()):
        n_samples = len(os.listdir(img_path))
        if label_path is not None:
            self.images = [f'{img_path}/{i+7000}.jpg' for i in range(n_samples)]
            self.labels = pd.read_csv(label_path)['label'].values.tolist()
        else:
            self.images = [f'{img_path}/{i}.jpg' for i in range(n_samples)]
            self.labels = None
        self.tfm = tfm

    def __getitem__(self, idx):
        img = Image.open(self.images[idx])
        img = self.tfm(img)
        if self.labels is not None:
            lab = torch.tensor(self.labels[idx]).long()
            return img, lab
        else:
            return img

    def __len__(self):
        return len(self.images)

# TODO: define your augmentation for training and evaluation
# 1. å®šç¾©ä¸€å€‹ç”¨æ–¼è¨ˆç®—çµ±è¨ˆé‡çš„åŸºç¤è½‰æ›
calc_tfm = T.Compose([
    T.Resize((64, 64)),
    T.Grayscale(num_output_channels=1),
    T.ToTensor(), # é€™ä¸€å±¤æœƒå°‡åƒç´ å€¼è½‰æ›åˆ° [0.0, 1.0] å€é–“
])

# 2. å»ºç«‹ä¸€å€‹è‡¨æ™‚çš„ Dataset å’Œ DataLoader (ä½¿ç”¨æ•´å€‹è¨“ç·´é›†)
# æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘ä½¿ç”¨å®Œæ•´çš„ train_datasetï¼Œä¸é€²è¡Œ train/valid åˆ†å‰²
full_train_dataset = FaceExpressionDataset(TRA_PATH, LABEL_PATH, tfm=calc_tfm)

# è¨­ç½®ä¸€å€‹è¶³å¤ å¤§çš„ BATCH_SIZE ä»¥åŠ å¿«è¨ˆç®—
calc_loader = DataLoader(full_train_dataset, batch_size=256, shuffle=False)

def calculate_mean_std(loader):
    """è¨ˆç®—è³‡æ–™é›†çš„ Mean å’Œ Stdã€‚"""
    
    channels = 1 # ç°åº¦åœ–
    # åˆå§‹åŒ–çµ±è¨ˆé‡
    sum_val = torch.zeros(channels)
    sum_sq_val = torch.zeros(channels)
    total_pixels = 0
    
    # éæ­·æ‰€æœ‰æ‰¹æ¬¡
    for images, _ in tqdm(loader, desc="Calculating Mean and Std"):
        # images çš„å½¢ç‹€: (BATCH, 1, H, W)
        
        # å°‡åœ–ç‰‡å±•å¹³ï¼Œåªä¿ç•™é€šé“ç¶­åº¦
        # ä¾‹ï¼š(256, 1, 48, 48) -> (256 * 48 * 48, 1)
        pixels = images.view(-1, channels) 
        
        # ç´¯ç©ç¸½å’Œ
        sum_val += pixels.sum(dim=0).cpu() # dim=0 åœ¨å±•å¹³å¾Œçš„è»¸ä¸Šæ±‚å’Œ
        
        # ç´¯ç©å¹³æ–¹ç¸½å’Œ
        sum_sq_val += (pixels ** 2).sum(dim=0).cpu()
        
        # ç´¯ç©åƒç´ ç¸½æ•¸é‡ (H * W * BATCH_SIZE)
        total_pixels += pixels.size(0)

    # æœ€çµ‚è¨ˆç®—
    mean = sum_val / total_pixels
    
    # æ–¹å·® (Variance) = (å¹³æ–¹ç¸½å’Œ / ç¸½åƒç´ æ•¸) - (å¹³å‡å€¼)^2
    variance = (sum_sq_val / total_pixels) - (mean ** 2)
    
    # æ¨™æº–å·® (Std) = sqrt(æ–¹å·®)
    std = torch.sqrt(variance)
    
    # å› ç‚ºæ˜¯å–®é€šé“ï¼Œçµæœæœƒæ˜¯ [mean_val] å’Œ [std_val]ï¼Œå–ç¬¬ä¸€å€‹å…ƒç´ å³å¯
    return mean.item(), std.item()

# åŸ·è¡Œè¨ˆç®—
dataset_mean, dataset_std = calculate_mean_std(calc_loader)

print(f"Dataset Mean (ç°åº¦): {dataset_mean:.4f}")
print(f"Dataset Std (ç°åº¦): {dataset_std:.4f}")

MEAN = [dataset_mean]  
STD = [dataset_std]

train_tfm = T.Compose([
    # ç°åº¦åœ–ç‰‡é€šå¸¸æ˜¯ 48x48ï¼Œä½†åŸå§‹è®€å–æ˜¯ 3 é€šé“ç°åº¦åœ–ï¼Œéœ€è¦è™•ç†
    T.Grayscale(num_output_channels=1),
    T.Resize((64, 64)),
    # 1. è³‡æ–™å¢å¼·
    T.RandomHorizontalFlip(p=0.5),  # å·¦å³ç¿»è½‰
    T.RandomRotation(15),           # éš¨æ©Ÿæ—‹è½‰
    T.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2)), # ä»¿å°„è®Šæ›
    
    # 2. è½‰æ›èˆ‡æ¨™æº–åŒ–
    T.ToTensor(),
    # T.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3)),
    
    # å°å–®é€šé“ç°åº¦åœ–é€²è¡Œæ¨™æº–åŒ– (è«‹ä½¿ç”¨æ‚¨è³‡æ–™é›†å¯¦éš›çš„ Mean/Stdï¼Œé€™è£¡ä½¿ç”¨å¸¸è¦‹å€¼)
    T.Normalize(mean=MEAN, std=STD),
])

eval_tfm = T.Compose([
    T.Grayscale(num_output_channels=1),
    T.Resize((64, 64)),
    T.ToTensor(),
    T.Normalize(mean=MEAN, std=STD),
])

# dataset
train_dataset = FaceExpressionDataset(TRA_PATH, LABEL_PATH, tfm=train_tfm)
train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [0.8, 0.2])
test_dataset = FaceExpressionDataset(TST_PATH, tfm=eval_tfm)
# dataloader
train_loader = DataLoader(train_dataset, batch_size=config.BATCH, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=config.BATCH, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=config.BATCH, shuffle=False)

"""#### Model"""

# TODO: define your CNN model
# class FaceExpressionNet(nn.Module):
#     def __init__(self):
#         super().__init__()

#         self.conv = nn.Sequential(
#             nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(kernel_size=2, stride=2)
#         )

#         self.fc = nn.Sequential(
#             nn.Linear(32 * 16 * 16, 128),
#             nn.ReLU(),
#             nn.Linear(128, 7)
#         )

#     def forward(self, x):
#         x = self.conv(x)
#         x = x.view(-1, 32 * 16 * 16)
#         x = self.fc(x)
#         return x
    
class FaceExpressionNet(nn.Module):
    def __init__(self):
        super().__init__()
        
        # LeakyReLU ä»£æ›¿ ReLUï¼Œä½¿æ¢¯åº¦åœ¨è² å€é–“ä¸æœƒå®Œå…¨ç‚ºé›¶
        self.relu = nn.LeakyReLU(0.1)

        # === Conv Block 1: å¯¬åº¦ 1 -> 64 ===
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64), self.relu,
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64), self.relu,
            nn.MaxPool2d(kernel_size=2, stride=2) # å°ºå¯¸: 64x64 -> 32x32
        )
        
        # === Conv Block 2: å¯¬åº¦ 64 -> 128 ===
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128), self.relu,
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128), self.relu,
            nn.MaxPool2d(kernel_size=2, stride=2) # å°ºå¯¸: 32x32 -> 16x16
        )
        
        # === Conv Block 3: å¯¬åº¦ 128 -> 256 ===
        self.conv3 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256), self.relu,
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256), self.relu,
            nn.MaxPool2d(kernel_size=2, stride=2) # å°ºå¯¸: 16x16 -> 8x8
        )
        
        # ğŸš€ å¯¦ä½œ GAP: Global Average Pooling
        # å°‡ 256 x 8 x 8 çš„ç‰¹å¾µåœ–è½‰æ›ç‚º 256 x 1 x 1
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        # å±•å¹³å¾Œå°ºå¯¸ï¼š256 (é€šé“æ•¸) * 1 * 1 = 256
        self.fc_layers = nn.Sequential(
            nn.Dropout(0.3), # é™ä½ Dropout æŠ‘åˆ¶ï¼Œè§£æ±º Underfitting
            nn.Linear(256, 128), 
            nn.BatchNorm1d(128),
            self.relu,
            nn.Dropout(0.3),
            nn.Linear(128, 7)
        )

    def forward(self, x):
        # 1. åŸ·è¡Œå·ç©å€å¡Š
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)

        # ğŸš€ ä¿®æ­£é» 1: å‘¼å« GAP å±¤
        # å°‡ x çš„å°ºå¯¸å¾ (B, 256, 8, 8) è½‰æ›ç‚º (B, 256, 1, 1)
        x = self.avg_pool(x)
        
        # ğŸš€ ä¿®æ­£é» 2: å±•å¹³
        # å°‡ x çš„å°ºå¯¸å¾ (B, 256, 1, 1) è½‰æ›ç‚º (B, 256)
        # é€™è£¡çš„ x.view(x.size(0), -1) æ‰æ˜¯æ­£ç¢ºçš„ GAP å¾Œå±•å¹³æ“ä½œ
        x = x.view(x.size(0), -1)
        
        # 3. åŸ·è¡Œå…¨é€£æ¥å€å¡Š
        x = self.fc_layers(x)
        return x

"""#### training loop"""

def train(model, train_loader, valid_loader, config):
    model.to(device)
    criteria = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)
    best_acc = 0
    train_losses, valid_losses = [], []
    for epoch in range(config.n_epoch):
        # train
        model.train()
        train_loss, train_acc = 0, 0
        for img, lab in tqdm(train_loader):
            img, lab = img.to(device), lab.to(device)
            output = model(img)
            optimizer.zero_grad()
            loss = criteria(output, lab)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            train_acc += (torch.argmax(output, dim=-1) == lab).float().mean().item()
        train_loss, train_acc = train_loss/len(train_loader), train_acc/len(train_loader)
        train_losses.append(train_loss)
        print(f'Epoch: {epoch+1}/{config.n_epoch}, train loss: {train_loss:.4f}, train acc: {train_acc:.4f}')

        # valid
        model.eval()
        valid_loss, valid_acc = 0, 0
        with torch.no_grad():
            for img, lab in valid_loader:
                img, lab = img.to(device), lab.to(device)
                output = model(img)
                loss = criteria(output, lab)
                valid_loss += loss.item()
                valid_acc += (torch.argmax(output, dim=-1) == lab).float().mean().item()
        valid_loss, valid_acc = valid_loss/len(valid_loader), valid_acc/len(valid_loader)
        valid_losses.append(valid_loss)
        print(f'Epoch: {epoch+1}/{config.n_epoch}, valid loss: {valid_loss:.4f}, valid acc: {valid_acc:.4f}')

        # update
        if valid_acc > best_acc:
            best_acc = valid_acc
            torch.save(model.state_dict(), config.ckpt_path)
            print(f'== best valid acc: {best_acc:.4f} ==')
        scheduler.step(valid_acc)
    model.load_state_dict(torch.load(config.ckpt_path))

    # plot the training/validation loss curve
    plt.figure(figsize=(8, 6))
    plt.plot(range(config.n_epoch), train_losses, label='Training Loss')
    plt.plot(range(config.n_epoch), valid_losses, label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

"""### Training"""

model = FaceExpressionNet()
train(model, train_loader, valid_loader, config)

# def draw_confusion_matrix(model, valid_loader):
#     predictions, labels = [], []
#     model.to(device)
#     model.eval()
#     with torch.no_grad():
#         for img, lab in tqdm(valid_loader):
#             img = img.to(device)
#             output = model(img)
#             predictions += torch.argmax(output, dim=-1).tolist()
#             labels += lab.tolist()
#     # TODO draw the confusion matrix
#     pass

# import numpy as np
# import matplotlib.pyplot as plt
# æ³¨æ„ï¼šé€™è£¡ä¸å†éœ€è¦ import sklearn.metrics æˆ– seaborn

def draw_confusion_matrix(model, valid_loader):
    predictions, labels = [], []
    model.to(device)
    model.eval()
    
    # 1. æ”¶é›†æ‰€æœ‰é æ¸¬å’ŒçœŸå¯¦æ¨™ç±¤
    with torch.no_grad():
        for img, lab in tqdm(valid_loader, desc="Collecting predictions"):
            img = img.to(device)
            output = model(img)
            predictions += torch.argmax(output, dim=-1).tolist()
            labels += lab.tolist()
            
    # å°‡åˆ—è¡¨è½‰æ›ç‚º NumPy é™£åˆ—
    y_true = np.array(labels)
    y_pred = np.array(predictions)

    # 2. æ‰‹å‹•è¨ˆç®—æ··æ·†çŸ©é™£ (ä»£æ›¿ sklearn.metrics.confusion_matrix)
    num_classes = 7 # æ‚¨çš„æƒ…ç·’é¡åˆ¥æ•¸é‡
    cm = np.zeros((num_classes, num_classes), dtype=int)
    
    for true_label, pred_label in zip(y_true, y_pred):
        # cm[i, j] æ˜¯çœŸå¯¦æ¨™ç±¤ i è¢«é æ¸¬ç‚º j çš„æ¬¡æ•¸
        cm[true_label, pred_label] += 1
        
    # å‡è¨­æƒ…ç·’é¡åˆ¥ (0-6) å°æ‡‰çš„æ¨™ç±¤åç¨±
    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

    # 3. ç¹ªè£½æ··æ·†çŸ©é™£
    plt.figure(figsize=(10, 8))
    
    # ä½¿ç”¨ imshow ç¹ªè£½é¡è‰²ç†±åŠ›åœ–
    # cmap=plt.cm.Blues æ˜¯ Matplotlib å…§å»ºçš„è—è‰²è‰²éš
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues) 
    plt.title('Confusion Matrix for Facial Expression Recognition')
    plt.colorbar() # æ·»åŠ é¡è‰²æ¢

    tick_marks = np.arange(num_classes)
    plt.xticks(tick_marks, emotion_labels, rotation=45) # è¨­ç½® X è»¸æ¨™ç±¤
    plt.yticks(tick_marks, emotion_labels)             # è¨­ç½® Y è»¸æ¨™ç±¤

    # 4. åœ¨æ¯å€‹æ ¼å­ä¸Šæ·»åŠ æ•¸å€¼æ¨™è¨» (Annotation)
    thresh = cm.max() / 2.0
    for i in range(num_classes):
        for j in range(num_classes):
            # æ ¹æ“šèƒŒæ™¯é¡è‰²ï¼Œèª¿æ•´æ–‡å­—é¡è‰² (æ·±è‰²èƒŒæ™¯ç”¨ç™½è‰²å­—ï¼Œæ·ºè‰²èƒŒæ™¯ç”¨é»‘è‰²å­—)
            color = "white" if cm[i, j] > thresh else "black"
            
            # ä½¿ç”¨ plt.text å‡½å¼ï¼Œå°‡æ•¸å€¼æ”¾ç½®åœ¨æ ¼å­ä¸­å¤®
            plt.text(j, i, format(cm[i, j], 'd'), # 'd' è¡¨ç¤ºæ•´æ•¸æ ¼å¼
                     horizontalalignment="center",
                     color=color)

    plt.tight_layout() # è‡ªå‹•èª¿æ•´ä½ˆå±€ï¼Œé¿å…æ¨™ç±¤é‡ç–Š
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

# ç¢ºä¿åœ¨è¨“ç·´å¾Œå‘¼å«
# model.load_state_dict(torch.load(config.ckpt_path)) # ç¢ºä¿è¼‰å…¥æœ€ä½³æ¨¡å‹
# draw_confusion_matrix(model, valid_loader)

draw_confusion_matrix(model, valid_loader)

"""### Testing"""

def test(model, test_loader):
    predictions = []
    model.to(device)
    model.eval()
    with torch.no_grad():
        for img in tqdm(test_loader):
            img = img.to(device)
            output = model(img)
            predictions += torch.argmax(output, dim=-1).tolist()
    with open('predict.csv', 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['id', 'label'])
        for id, r in enumerate(predictions):
            writer.writerow([id, r])

model.load_state_dict(torch.load('model.pth'))
test(model, test_loader)