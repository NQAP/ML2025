# -*- coding: utf-8 -*-
import os
import random
import csv
import torch
import torch.nn as nn
import numpy as np
import pandas as pd

import torchvision.transforms as T
import matplotlib.pyplot as plt

from torch.utils.data import Dataset, DataLoader
from PIL import Image
from argparse import Namespace
from tqdm import tqdm

"""2025ML_HW2_sample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YT4zCSCfrPnry01kM-xI2brJYQUuE6rz

# ML HW2 sample code
TODO:
 - Design your CNN model (you are welcomed to use existing models)
 - Hyperparameter tuning
 - Confusion matrix

Report:
 - Structure of CNN model
 - Data Augmentation
 - Draw the confusion matrix

#### Download data
"""

# !gdown 19oEShkdcBJf41nu46anyvu7BgXUffIGi
# !unzip -q 'HW2.zip'

"""#### Import packages"""


"""#### Hyperparameters and setting"""

# TODO: modify the hyperparameters
config = Namespace(
    random_seed = 42,
    BATCH = 128,
    n_epoch = 30,
    lr = 1.5e-3,
    weight_decay = 1e-5,
    ckpt_path = 'model.pth',
)

TRA_PATH = 'data/train/'
TST_PATH = 'data/test/'
LABEL_PATH = 'data/train.csv'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.manual_seed(config.random_seed)
torch.cuda.manual_seed_all(config.random_seed)
random.seed(config.random_seed)
np.random.seed(config.random_seed)

"""#### Dataset and Dataloader"""

class FaceExpressionDataset(Dataset):
    def __init__(self, img_path, label_path=None, tfm=T.ToTensor()):
        n_samples = len(os.listdir(img_path))
        if label_path is not None:
            self.images = [f'{img_path}/{i+7000}.jpg' for i in range(n_samples)]
            self.labels = pd.read_csv(label_path)['label'].values.tolist()
        else:
            self.images = [f'{img_path}/{i}.jpg' for i in range(n_samples)]
            self.labels = None
        self.tfm = tfm

    def __getitem__(self, idx):
        img = Image.open(self.images[idx])
        img = self.tfm(img)
        if self.labels is not None:
            lab = torch.tensor(self.labels[idx]).long()
            return img, lab
        else:
            return img

    def __len__(self):
        return len(self.images)

# TODO: define your augmentation for training and evaluation
# 1. 定義一個用於計算統計量的基礎轉換
calc_tfm = T.Compose([
    T.Resize((64, 64)),
    T.Grayscale(num_output_channels=1),
    T.ToTensor(), # 這一層會將像素值轉換到 [0.0, 1.0] 區間
])

# 2. 建立一個臨時的 Dataset 和 DataLoader (使用整個訓練集)
# 注意：這裡我們使用完整的 train_dataset，不進行 train/valid 分割
full_train_dataset = FaceExpressionDataset(TRA_PATH, LABEL_PATH, tfm=calc_tfm)

# 設置一個足夠大的 BATCH_SIZE 以加快計算
calc_loader = DataLoader(full_train_dataset, batch_size=256, shuffle=False)

def calculate_mean_std(loader):
    """計算資料集的 Mean 和 Std。"""
    
    channels = 1 # 灰度圖
    # 初始化統計量
    sum_val = torch.zeros(channels)
    sum_sq_val = torch.zeros(channels)
    total_pixels = 0
    
    # 遍歷所有批次
    for images, _ in tqdm(loader, desc="Calculating Mean and Std"):
        # images 的形狀: (BATCH, 1, H, W)
        
        # 將圖片展平，只保留通道維度
        # 例：(256, 1, 48, 48) -> (256 * 48 * 48, 1)
        pixels = images.view(-1, channels) 
        
        # 累積總和
        sum_val += pixels.sum(dim=0).cpu() # dim=0 在展平後的軸上求和
        
        # 累積平方總和
        sum_sq_val += (pixels ** 2).sum(dim=0).cpu()
        
        # 累積像素總數量 (H * W * BATCH_SIZE)
        total_pixels += pixels.size(0)

    # 最終計算
    mean = sum_val / total_pixels
    
    # 方差 (Variance) = (平方總和 / 總像素數) - (平均值)^2
    variance = (sum_sq_val / total_pixels) - (mean ** 2)
    
    # 標準差 (Std) = sqrt(方差)
    std = torch.sqrt(variance)
    
    # 因為是單通道，結果會是 [mean_val] 和 [std_val]，取第一個元素即可
    return mean.item(), std.item()

# 執行計算
dataset_mean, dataset_std = calculate_mean_std(calc_loader)

print(f"Dataset Mean (灰度): {dataset_mean:.4f}")
print(f"Dataset Std (灰度): {dataset_std:.4f}")

MEAN = [dataset_mean]  
STD = [dataset_std]

train_tfm = T.Compose([
    # 灰度圖片通常是 48x48，但原始讀取是 3 通道灰度圖，需要處理
    T.Grayscale(num_output_channels=1),
    T.Resize((64, 64)),
    # 1. 資料增強
    T.RandomHorizontalFlip(p=0.5),  # 左右翻轉
    T.RandomRotation(15),           # 隨機旋轉
    T.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2)), # 仿射變換
    
    # 2. 轉換與標準化
    T.ToTensor(),
    # T.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3)),
    
    # 對單通道灰度圖進行標準化 (請使用您資料集實際的 Mean/Std，這裡使用常見值)
    T.Normalize(mean=MEAN, std=STD),
])

eval_tfm = T.Compose([
    T.Grayscale(num_output_channels=1),
    T.Resize((64, 64)),
    T.ToTensor(),
    T.Normalize(mean=MEAN, std=STD),
])

# dataset
train_dataset = FaceExpressionDataset(TRA_PATH, LABEL_PATH, tfm=train_tfm)
train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [0.8, 0.2])
test_dataset = FaceExpressionDataset(TST_PATH, tfm=eval_tfm)
# dataloader
train_loader = DataLoader(train_dataset, batch_size=config.BATCH, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=config.BATCH, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=config.BATCH, shuffle=False)

"""#### Model"""

# TODO: define your CNN model
# class FaceExpressionNet(nn.Module):
#     def __init__(self):
#         super().__init__()

#         self.conv = nn.Sequential(
#             nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(kernel_size=2, stride=2)
#         )

#         self.fc = nn.Sequential(
#             nn.Linear(32 * 16 * 16, 128),
#             nn.ReLU(),
#             nn.Linear(128, 7)
#         )

#     def forward(self, x):
#         x = self.conv(x)
#         x = x.view(-1, 32 * 16 * 16)
#         x = self.fc(x)
#         return x
    
class FaceExpressionNet(nn.Module):
    def __init__(self):
        super().__init__()
        
        # LeakyReLU 代替 ReLU，使梯度在負區間不會完全為零
        self.relu = nn.LeakyReLU(0.1)

        # === Conv Block 1: 寬度 1 -> 64 ===
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64), self.relu,
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64), self.relu,
            nn.MaxPool2d(kernel_size=2, stride=2) # 尺寸: 64x64 -> 32x32
        )
        
        # === Conv Block 2: 寬度 64 -> 128 ===
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128), self.relu,
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128), self.relu,
            nn.MaxPool2d(kernel_size=2, stride=2) # 尺寸: 32x32 -> 16x16
        )
        
        # === Conv Block 3: 寬度 128 -> 256 ===
        self.conv3 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256), self.relu,
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256), self.relu,
            nn.MaxPool2d(kernel_size=2, stride=2) # 尺寸: 16x16 -> 8x8
        )
        
        # 🚀 實作 GAP: Global Average Pooling
        # 將 256 x 8 x 8 的特徵圖轉換為 256 x 1 x 1
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        # 展平後尺寸：256 (通道數) * 1 * 1 = 256
        self.fc_layers = nn.Sequential(
            nn.Dropout(0.3), # 降低 Dropout 抑制，解決 Underfitting
            nn.Linear(256, 128), 
            nn.BatchNorm1d(128),
            self.relu,
            nn.Dropout(0.3),
            nn.Linear(128, 7)
        )

    def forward(self, x):
        # 1. 執行卷積區塊
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)

        # 🚀 修正點 1: 呼叫 GAP 層
        # 將 x 的尺寸從 (B, 256, 8, 8) 轉換為 (B, 256, 1, 1)
        x = self.avg_pool(x)
        
        # 🚀 修正點 2: 展平
        # 將 x 的尺寸從 (B, 256, 1, 1) 轉換為 (B, 256)
        # 這裡的 x.view(x.size(0), -1) 才是正確的 GAP 後展平操作
        x = x.view(x.size(0), -1)
        
        # 3. 執行全連接區塊
        x = self.fc_layers(x)
        return x

"""#### training loop"""

def train(model, train_loader, valid_loader, config):
    model.to(device)
    criteria = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)
    best_acc = 0
    train_losses, valid_losses = [], []
    for epoch in range(config.n_epoch):
        # train
        model.train()
        train_loss, train_acc = 0, 0
        for img, lab in tqdm(train_loader):
            img, lab = img.to(device), lab.to(device)
            output = model(img)
            optimizer.zero_grad()
            loss = criteria(output, lab)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            train_acc += (torch.argmax(output, dim=-1) == lab).float().mean().item()
        train_loss, train_acc = train_loss/len(train_loader), train_acc/len(train_loader)
        train_losses.append(train_loss)
        print(f'Epoch: {epoch+1}/{config.n_epoch}, train loss: {train_loss:.4f}, train acc: {train_acc:.4f}')

        # valid
        model.eval()
        valid_loss, valid_acc = 0, 0
        with torch.no_grad():
            for img, lab in valid_loader:
                img, lab = img.to(device), lab.to(device)
                output = model(img)
                loss = criteria(output, lab)
                valid_loss += loss.item()
                valid_acc += (torch.argmax(output, dim=-1) == lab).float().mean().item()
        valid_loss, valid_acc = valid_loss/len(valid_loader), valid_acc/len(valid_loader)
        valid_losses.append(valid_loss)
        print(f'Epoch: {epoch+1}/{config.n_epoch}, valid loss: {valid_loss:.4f}, valid acc: {valid_acc:.4f}')

        # update
        if valid_acc > best_acc:
            best_acc = valid_acc
            torch.save(model.state_dict(), config.ckpt_path)
            print(f'== best valid acc: {best_acc:.4f} ==')
        scheduler.step(valid_acc)
    model.load_state_dict(torch.load(config.ckpt_path))

    # plot the training/validation loss curve
    plt.figure(figsize=(8, 6))
    plt.plot(range(config.n_epoch), train_losses, label='Training Loss')
    plt.plot(range(config.n_epoch), valid_losses, label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

"""### Training"""

model = FaceExpressionNet()
train(model, train_loader, valid_loader, config)

# def draw_confusion_matrix(model, valid_loader):
#     predictions, labels = [], []
#     model.to(device)
#     model.eval()
#     with torch.no_grad():
#         for img, lab in tqdm(valid_loader):
#             img = img.to(device)
#             output = model(img)
#             predictions += torch.argmax(output, dim=-1).tolist()
#             labels += lab.tolist()
#     # TODO draw the confusion matrix
#     pass

# import numpy as np
# import matplotlib.pyplot as plt
# 注意：這裡不再需要 import sklearn.metrics 或 seaborn

def draw_confusion_matrix(model, valid_loader):
    predictions, labels = [], []
    model.to(device)
    model.eval()
    
    # 1. 收集所有預測和真實標籤
    with torch.no_grad():
        for img, lab in tqdm(valid_loader, desc="Collecting predictions"):
            img = img.to(device)
            output = model(img)
            predictions += torch.argmax(output, dim=-1).tolist()
            labels += lab.tolist()
            
    # 將列表轉換為 NumPy 陣列
    y_true = np.array(labels)
    y_pred = np.array(predictions)

    # 2. 手動計算混淆矩陣 (代替 sklearn.metrics.confusion_matrix)
    num_classes = 7 # 您的情緒類別數量
    cm = np.zeros((num_classes, num_classes), dtype=int)
    
    for true_label, pred_label in zip(y_true, y_pred):
        # cm[i, j] 是真實標籤 i 被預測為 j 的次數
        cm[true_label, pred_label] += 1
        
    # 假設情緒類別 (0-6) 對應的標籤名稱
    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

    # 3. 繪製混淆矩陣
    plt.figure(figsize=(10, 8))
    
    # 使用 imshow 繪製顏色熱力圖
    # cmap=plt.cm.Blues 是 Matplotlib 內建的藍色色階
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues) 
    plt.title('Confusion Matrix for Facial Expression Recognition')
    plt.colorbar() # 添加顏色條

    tick_marks = np.arange(num_classes)
    plt.xticks(tick_marks, emotion_labels, rotation=45) # 設置 X 軸標籤
    plt.yticks(tick_marks, emotion_labels)             # 設置 Y 軸標籤

    # 4. 在每個格子上添加數值標註 (Annotation)
    thresh = cm.max() / 2.0
    for i in range(num_classes):
        for j in range(num_classes):
            # 根據背景顏色，調整文字顏色 (深色背景用白色字，淺色背景用黑色字)
            color = "white" if cm[i, j] > thresh else "black"
            
            # 使用 plt.text 函式，將數值放置在格子中央
            plt.text(j, i, format(cm[i, j], 'd'), # 'd' 表示整數格式
                     horizontalalignment="center",
                     color=color)

    plt.tight_layout() # 自動調整佈局，避免標籤重疊
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

# 確保在訓練後呼叫
# model.load_state_dict(torch.load(config.ckpt_path)) # 確保載入最佳模型
# draw_confusion_matrix(model, valid_loader)

draw_confusion_matrix(model, valid_loader)

"""### Testing"""

def test(model, test_loader):
    predictions = []
    model.to(device)
    model.eval()
    with torch.no_grad():
        for img in tqdm(test_loader):
            img = img.to(device)
            output = model(img)
            predictions += torch.argmax(output, dim=-1).tolist()
    with open('predict.csv', 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['id', 'label'])
        for id, r in enumerate(predictions):
            writer.writerow([id, r])

model.load_state_dict(torch.load('model.pth'))
test(model, test_loader)