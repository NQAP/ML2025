# -*- coding: utf-8 -*-
"""2025ML_HW4_sample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfYMQZdomFow5WZzQ-McS85xLBO6JFuW

# ML HW4 Sample Code
TODO:
 - Design your LSTM model
 - Use unlabelled data (train_nolabel.csv) for Word2Vec training
    - Combine labeled + unlabeled data to train better embeddings
 - Train with labelled data (train_label.csv)
    - Optional: Data augmentation
    - Optional: Custom loss function

## Download data
"""



"""## Import packages"""

import torch
import os
import csv
import random
import re
import numpy as np
import pandas as pd
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from gensim.models import Word2Vec
from gensim.models.phrases import Phrases, Phraser
from gensim.models import FastText  # 引入 FastText
from sklearn.model_selection import train_test_split

"""## Set the Configurations"""

# !pip install -U gdown -q
# !gdown --folder https://drive.google.com/drive/folders/1786AXJRAtqFvWMBeh-bLm4MtU21IQpBg
# !pip install gensim
# Training Config
DEVICE_NUM = 2
BATCH_SIZE = 64
EPOCH_NUM = 50
MAX_POSITIONS_LEN = 500
SEED = 0
MODEL_DIR = 'model.pth'
lr = 0.001

# Set Seed
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
random.seed(SEED)
np.random.seed(SEED)

# torch.cuda.set_device(0)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# RNN Config
w2v_config = {'path': 'model_pseudo', 'dim': 256}

lstm_config = {
    'hidden_dim': 128,
    'num_layers': 1,        # 有了 Pack 和 Attention，1 層通常就夠強且夠快
    'bidirectional': True,
    'fix_embedding': False  # 建議解凍，因為我們有 Unlabeled Data 強化過 Word2Vec
}

header_config = {
    'dropout': 0.2,         # 因為解凍了 Embedding，Dropout 可以高一點防止過擬合
    'hidden_dim': 256       # 256 * 2
}

# 確保維度對應正確 (這行原本的 assert 很好，留著)
assert header_config['hidden_dim'] == lstm_config['hidden_dim'] or header_config['hidden_dim'] == lstm_config['hidden_dim'] * 2

"""## Utils for Datasets and Dataloaders"""

# 把 Regex 編譯放在函式外面，解決你之前遇到的卡頓問題
REGEX_URL = re.compile(r'http\S+')
REGEX_USER = re.compile(r'@\w+')
REGEX_HASHTAG = re.compile(r'#')
REGEX_REPEAT = re.compile(r'(.)\1{2,}')  # 抓出重複3次以上的字
REGEX_PUNCTUATION = re.compile(r'([^\w\s])') # 抓出所有非文字、非空白的符號
REGEX_SPACES = re.compile(r'\s+')

def parsing_text(text):
    if text is None or pd.isna(text):
        return ""
    
    text = str(text).lower() 

    # --- [新增] 先保護表情符號 ---
    # 定義字典映射
    emojis = {
        ':)': ' _smile_ ', ':-)': ' _smile_ ', ';)': ' _wink_ ', 
        ':(': ' _sad_ ',   ':-(': ' _sad_ ',  ':/': ' _worry_ ',
        '<3': ' _love_ '
    }
    for emo, token in emojis.items():
        text = text.replace(emo, token)
    # ---------------------------

    text = REGEX_URL.sub('', text)
    
    # 1. 移除網址與 User ID (這些對情緒沒幫助)
    text = REGEX_URL.sub('', text)
    text = REGEX_USER.sub('', text)
    text = REGEX_HASHTAG.sub('', text) # 移除 # 符號，保留後面的文字
    
    # 2. 處理縮寫 (Contractions)
    # 手動加上空格，讓 .split(' ') 能把它們切開
    # 這對 Word2Vec 很重要，讓它學到 n't 就是 not
    text = text.replace("n't", " n't")
    text = text.replace("'s", " 's")
    text = text.replace("'m", " 'm")
    text = text.replace("'re", " 're")
    text = text.replace("'ve", " 've")
    text = text.replace("'ll", " 'll")
    text = text.replace("'d", " 'd")

    # 3. 處理重複字元 (Normalization)
    # 例如: "sooooo" -> "soo", "coool" -> "cool"
    # 邏輯：保留前兩個重複字元，後面的刪掉
    text = REGEX_REPEAT.sub(r'\1\1', text)

    # 4. [關鍵] 保留標點符號與表情
    # 原本是直接刪除，現在改成 "在前後加空白"
    # "good!" -> "good ! "
    # ":)" -> " : ) "
    # 這樣 split 後，"!" 和 ":" 就會變成獨立的 Token，模型就能學到它們的情緒權重
    text = REGEX_PUNCTUATION.sub(r' \1 ', text)
    
    # 5. 整理多餘空白
    text = REGEX_SPACES.sub(' ', text).strip()
    
    return text

def load_train_label(path='datatrain_label.csv'):
    tra_lb_pd = pd.read_csv(path)
    idx = tra_lb_pd['id'].tolist()
    text = [parsing_text(s).split() for s in tra_lb_pd['text'].tolist()]
    label = tra_lb_pd['label'].tolist()
    return idx, text, label

def load_train_nolabel(path='dataset/train_nolabel.csv'):
    tra_nlb_pd = pd.read_csv(path)
    # [修改關鍵] 這裡必須多回傳一個 idx，因為 Pseudo-labeling 需要 ID
    idx = [f'nolabel_{i}' for i in range(len(tra_nlb_pd))]
    text = [parsing_text(s).split() for s in tra_nlb_pd['text'].tolist()]
    
    # 回傳兩個值：ID列表, 文字列表
    return idx, text

def load_test(path='test.csv'):
    test_pd = pd.read_csv(path)
    idx = test_pd['id'].tolist()
    text = [parsing_text(s).split() for s in test_pd['text'].tolist()]
    return idx, text

"""## Datasets and Dataloaders"""


class Preprocessor:
    def __init__(self, sentence_iterator, w2v_config):
        self.word2idx = {}
        self.idx2word = []
        self.embedding_matrix = []
        
        # [新增 1] 訓練 Bigram 模型 (自動偵測常見詞組)
        print("Training Bigram detector...")
        # min_count=5: 至少出現5次才算片語
        # threshold=10: 數值越高越嚴格 (越高代表兩個字黏得越緊)
        phrases = Phrases(sentence_iterator, min_count=5, threshold=10)
        self.bigram_transformer = Phraser(phrases)
        
        # [新增 2] 轉換原本的句子 (把 "not", "good" 變成 "not_good")
        # 我們寫一個生成器來轉換，避免吃記憶體
        class BigramIterator:
            def __init__(self, iterator, transformer):
                self.iterator = iterator
                self.transformer = transformer
            def __iter__(self):
                for sentence in self.iterator:
                    # 這行會把 ["new", "york"] 變成 ["new_york"]
                    yield self.transformer[sentence]

        bigram_sentences = BigramIterator(sentence_iterator, self.bigram_transformer)
        
        # 用轉換後的句子訓練 Word2Vec
        self.build_word2vec(bigram_sentences, **w2v_config)

    def build_word2vec(self, x, path, dim):
        if os.path.isfile(path):
            print("loading FastText model ...")
            # 注意這裡載入方式也不同
            w2v_model = FastText.load(path)
        else:
            print("training FastText model ...")
            # min_n, max_n 是字根長度範圍，Twitter 短字多，3-6 是好選擇
            w2v_model = FastText(vector_size=dim, window=5, min_count=3, 
                                workers=12, epochs=10, seed=SEED,
                                min_n=3, max_n=6) 
            w2v_model.build_vocab(x)
            w2v_model.train(x, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)
            
            print("saving FastText model ...")
            w2v_model.save(path)

        # 後面邏輯一樣，FastText 兼容 Word2Vec 的 wv 介面
        self.embedding_dim = w2v_model.vector_size
        self.embedding_dim = w2v_model.vector_size
        for i, word in enumerate(w2v_model.wv.key_to_index):
            #e.g. self.word2index['he'] = 1
            #e.g. self.index2word[1] = 'he'
            #e.g. self.vectors[1] = 'he' vector

            self.word2idx[word] = len(self.word2idx)
            self.idx2word.append(word)
            self.embedding_matrix.append(w2v_model.wv[word])

        self.embedding_matrix = torch.tensor(self.embedding_matrix)
        self.add_embedding('<PAD>')
        self.add_embedding('<UNK>')
        print("total words: {}".format(len(self.embedding_matrix)))

    def add_embedding(self, word):
        vector = torch.empty(1, self.embedding_dim)
        torch.nn.init.uniform_(vector)
        self.word2idx[word] = len(self.word2idx)
        self.idx2word.append(word)
        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)

    def sentence2idx(self, sentence):
        # 進來前先把句子轉成 Bigram
        sentence = self.bigram_transformer[sentence] 
        
        sentence_idx = []
        for word in sentence:
            if word in self.word2idx.keys():
                sentence_idx.append(self.word2idx[word])
            else:
                sentence_idx.append(self.word2idx["<UNK>"])
        return torch.LongTensor(sentence_idx)

class SentenceIterator:
    def __init__(self, labeled_data, unlabeled_path):
        self.labeled_data = labeled_data  # 這是已經讀到記憶體裡的 labeled list
        self.unlabeled_path = unlabeled_path

    def __iter__(self):
        # 1. 先回傳有標註的資料 (已經在 RAM 裡了)
        for sentence in self.labeled_data:
            yield sentence
            
        # 2. 接著從硬碟串流讀取無標註資料 (省記憶體關鍵！)
        import csv
        # 使用標準 csv library 逐行讀取，不要用 pandas 一次讀
        with open(self.unlabeled_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader) # 跳過 header ('id', 'text')
            
            for row in reader:
                # 確保格式正確，row[1] 是 text
                if len(row) > 1:
                    # 這裡記得要用跟 labeled data 一樣的處理方式
                    text = parsing_text(row[1])
                    yield text.split(' ')

class TwitterDataset(torch.utils.data.Dataset):
    # [修改] 這裡新增了 filter_short 參數，預設為 True
    def __init__(self, id_list, sentences, labels, preprocessor, filter_short=True):
        self.data = []
        self.sentences = sentences
        self.labels = labels
        self.id_list = id_list
        self.preprocessor = preprocessor
        
        MIN_LENGTH = 3 
        
        for i in range(len(sentences)):
            input_ids = preprocessor.sentence2idx(sentences[i])
            
            # [邏輯修改] 
            # 如果 filter_short=True (訓練模式)，才執行長度檢查 (去雜訊)
            # 如果 filter_short=False (測試模式)，不管多短都要留著 (避免 Submission Error)
            if not filter_short or len(input_ids) >= MIN_LENGTH:
                
                # [保護機制] 萬一在 Test Set 中句子真的是空的 (長度0)，補一個 <UNK>
                if len(input_ids) == 0:
                     # 假設 0 是 <UNK> 或 <PAD> 的 index
                     input_ids = torch.LongTensor([preprocessor.word2idx.get("<UNK>", 0)])

                label = labels[i] if labels is not None else None
                self.data.append({
                    'id': id_list[i],
                    'text': input_ids,
                    'label': label
                })
        
        if filter_short:
            print(f"Filtered {len(sentences) - len(self.data)} short/empty samples (Training Mode).")
        else:
            print(f"Kept all {len(self.data)} samples (Test/Valid Mode).")

    def __getitem__(self, idx):
        if self.labels is None: 
            return self.data[idx]['id'], self.data[idx]['text']
        return self.data[idx]['id'], self.data[idx]['text'], self.data[idx]['label']

    def __len__(self):
        return len(self.data)

    def collate_fn(self, data):
        id_list = [d[0] for d in data]
        texts = [d[1] for d in data]
        lengths = torch.LongTensor([len(t) for t in texts])
        
        texts_padded = pad_sequence(texts, batch_first=True).contiguous()

        # 判斷有沒有 label
        if len(data[0]) == 2: # No label (Test set)
            return id_list, lengths, texts_padded

        labels = torch.FloatTensor([d[2] for d in data])
        return id_list, lengths, texts_padded, labels

"""## RNN Backbone"""

class LSTM_Backbone(torch.nn.Module):
    def __init__(self, embedding, hidden_dim, num_layers, bidirectional, fix_embedding=False):
        super(LSTM_Backbone, self).__init__()
        self.embedding = torch.nn.Embedding(embedding.size(0), embedding.size(1))
        self.embedding.weight = torch.nn.Parameter(embedding)
        self.embedding.weight.requires_grad = False if fix_embedding else True
        
        # [新增] Spatial Dropout (雖然是用 Dropout2d 實作，但對 Text 是一樣的效果)
        # 建議設 0.3 ~ 0.4
        self.embedding_dropout = torch.nn.Dropout2d(0.3) 

        self.lstm = torch.nn.LSTM(embedding.size(1), hidden_dim, num_layers=num_layers, \
                                  bidirectional=bidirectional, batch_first=True)

    def forward(self, inputs, lengths):
        # inputs: (Batch, Seq_Len)
        embeds = self.embedding(inputs) # (Batch, Seq_Len, Dim)
        
        # [新增] 實作 Spatial Dropout
        # Dropout2d 期望輸入是 (Batch, Channel, Height, Width)
        # 我們把 Dim 當作 Channel
        if self.training:
            # 1. 轉置: (Batch, Dim, Seq_Len)
            embeds = embeds.permute(0, 2, 1)  
            # 2. 增加一個維度變成 (Batch, Dim, Seq_Len, 1) 騙過 Dropout2d
            embeds = embeds.unsqueeze(3)
            # 3. 丟棄整條 Feature
            embeds = self.embedding_dropout(embeds)
            # 4. 還原形狀: (Batch, Seq_Len, Dim)
            embeds = embeds.squeeze(3).permute(0, 2, 1)
        
        # 接下來是原本的 Pack logic
        packed_input = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths.cpu(), batch_first=True, enforce_sorted=False)
        packed_output, (hidden, cell) = self.lstm(packed_input)
        out, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)
        
        return out

class Header(torch.nn.Module):
    def __init__(self, dropout, hidden_dim, num_heads=4):
        super(Header, self).__init__()
        
        self.num_heads = num_heads
        self.hidden_dim = hidden_dim
        
        # 確保 hidden_dim 能被 num_heads 整除
        assert hidden_dim % num_heads == 0
        self.head_dim = hidden_dim // num_heads
        
        # [模組 1] 多頭 Attention
        # 我們用一個大的 Linear 層算出所有頭的分數，然後再切開
        self.attention_linear = torch.nn.Linear(hidden_dim, num_heads)
        
        # [模組 2] 最終分類器
        # 因為有 num_heads 個頭，每個頭產出 head_dim 的向量
        # 拼接後維度依然是 hidden_dim
        self.classifier = torch.nn.Sequential(
            torch.nn.LayerNorm(hidden_dim),  # 改用 LayerNorm
            torch.nn.Dropout(dropout),
            
            torch.nn.Linear(hidden_dim, hidden_dim // 2),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout),
            
            torch.nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, inputs, lengths):
        # inputs: (Batch, Seq_Len, Hidden)
        batch_size = inputs.size(0)
        seq_len = inputs.size(1)
        
        # 1. 計算每個頭的 Attention Score
        # scores: (Batch, Seq_Len, Num_Heads)
        scores = self.attention_linear(inputs)
        
        # 2. 製作 Mask
        # (Batch, Seq_Len, 1) -> 廣播到 (Batch, Seq_Len, Num_Heads)
        mask = torch.arange(seq_len, device=inputs.device)[None, :] < lengths[:, None]
        mask = mask.unsqueeze(-1).expand(-1, -1, self.num_heads)
        
        # 3. Apply Mask & Softmax
        scores = scores.masked_fill(~mask, -1e9)
        # 在 Seq_Len 維度做 Softmax
        weights = F.softmax(scores, dim=1) # (Batch, Seq_Len, Num_Heads)
        
        # 4. 加權總和 (Weighted Sum) - 這是最 tricky 的部分
        # 我們希望每個頭只負責 input 的一部分特徵 (head_dim)
        
        # 先把 input 切成多個頭: (Batch, Seq_Len, Num_Heads, Head_Dim)
        inputs_reshaped = inputs.view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # 把 weights 擴展維度以便相乘: (Batch, Seq_Len, Num_Heads, 1)
        weights_expanded = weights.unsqueeze(-1)
        
        # 相乘並加總: (Batch, Seq_Len, Num_Heads, Head_Dim) -> Sum over Seq_Len
        # Result: (Batch, Num_Heads, Head_Dim)
        context_vectors = torch.sum(inputs_reshaped * weights_expanded, dim=1)
        
        # 5. 拼接 (Concatenate)
        # 把所有頭接回來: (Batch, Num_Heads * Head_Dim) -> (Batch, Hidden)
        context_vectors = context_vectors.view(batch_size, -1)
        
        # 6. 分類
        out = self.classifier(context_vectors).squeeze()
        
        return out
    
"""## Training & Validation"""

# 定義 KL Divergence Loss
def compute_kl_loss(p, q, pad_mask=None):
    p_loss = F.kl_div(F.log_softmax(p, dim=-1), F.softmax(q, dim=-1), reduction='none')
    q_loss = F.kl_div(F.log_softmax(q, dim=-1), F.softmax(p, dim=-1), reduction='none')
    
    # pad_mask is optional, generally average over batch is fine
    if pad_mask is not None:
        p_loss.masked_fill_(pad_mask, 0.)
        q_loss.masked_fill_(pad_mask, 0.)

    p_loss = p_loss.sum()
    q_loss = q_loss.sum()

    loss = (p_loss + q_loss) / 2
    return loss

class BinaryFocalLoss(torch.nn.Module):
    """
    Focal Loss for Binary Classification
    Formula: Loss = - alpha * (1 - p_t)^gamma * log(p_t)
    """
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super(BinaryFocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, logits, targets):
        # logits: (Batch_Size, ), 未經過 Sigmoid
        # targets: (Batch_Size, ), 0 或 1
        
        # 1. 計算原本的 Binary Cross Entropy
        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
        
        # 2. 取得預測機率 p_t
        # 如果 target=1, p_t = p; 如果 target=0, p_t = 1-p
        p = torch.sigmoid(logits)
        p_t = p * targets + (1 - p) * (1 - targets)
        
        # 3. 計算調節因子 (Modulating Factor): (1 - p_t)^gamma
        # 當 p_t 接近 1 (預測準確)，這個因子會趨近 0 -> 降低權重
        # 當 p_t 接近 0 (預測錯誤/難樣本)，這個因子會很大 -> 保持權重
        loss = self.alpha * (1 - p_t)**self.gamma * bce_loss
        
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss
        
# 修改 train 函數
def train(train_loader, backbone, header, optimizer, criterion, device, epoch):
    alpha = 2 # R-Drop 的權重 (通常設 1~5)

    for i, (idx_list, lengths, texts, labels) in enumerate(train_loader):
        lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)
        optimizer.zero_grad()
        
        # --- R-Drop 核心邏輯 ---
        # 1. 把 inputs 複製一份，變成兩倍大 (Batch*2, Seq_Len)
        inputs_doubled = torch.cat([inputs, inputs], dim=0)
        lengths_doubled = torch.cat([lengths, lengths], dim=0)
        
        # 2. 過模型 (因為有 Dropout，所以前半段和後半段出來的結果會不同)
        if backbone is not None:
            feats = backbone(inputs_doubled, lengths_doubled)
        logits = header(feats, lengths_doubled) # 注意：Header 最後不能有 Sigmoid
        
        # 3. 切開結果
        batch_size = inputs.size(0)
        logits1 = logits[:batch_size]
        logits2 = logits[batch_size:]
        soft_predicted = (logits1 + logits2) / 2
        # 4. 計算原本的 Loss (兩次都要算)
        # 注意：Loss Function 必須是 BCEWithLogitsLoss
        loss_nll = (criterion(logits1, labels) + criterion(logits2, labels)) / 2
        
        # 5. 計算 KL Loss (強迫兩次預測一致)
        # 對於二元分類，可以簡化為 MSE 或直接用 logits 算
        # 這裡為了簡單，我們用 MSE 近似 KL (對 Logits)
        loss_kl = compute_kl_loss(logits1, logits2) 
        
        # 總 Loss
        loss = loss_nll + alpha * loss_kl
        
        loss.backward()

        torch.nn.utils.clip_grad_norm_(backbone.parameters(), max_norm=1.0) # 建議設 1.0 或 5.0
        
        optimizer.step()
        

        with torch.no_grad():
            # [修正] 如果 Header 移除了 Sigmoid (搭配 BCEWithLogitsLoss)，這裡要改用 >= 0
            # 如果你 Header 還留著 Sigmoid，這裡維持 >= 0.5
            hard_predicted = (soft_predicted >= 0).int() 
            correct = sum(hard_predicted == labels).item()
            batch_size = len(labels)
            print('[ Epoch {}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(epoch+1,i+1, len(train_loader), loss.item(), correct * 100 / batch_size), end='\r')


def valid(valid_loader, backbone, header, criterion, device, epoch):
    backbone.eval()
    header.eval()
    with torch.no_grad():
        total_loss = []
        total_acc = []

        for i, (idx_list, lengths, texts, labels) in enumerate(valid_loader):
            lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)

            if not backbone is None:
                # [修正] 這裡要多傳入 lengths
                inputs = backbone(inputs, lengths)
                
            soft_predicted = header(inputs, lengths)
            loss = criterion(soft_predicted, labels)
            total_loss.append(loss.item())

            # [修正] 同上，配合 Loss Function 決定是 0 或 0.5
            hard_predicted = (soft_predicted >= 0).int()
            correct = sum(hard_predicted == labels).item()
            acc = correct * 100 / len(labels)
            total_acc.append(acc)

            print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f}'.format(epoch+1, np.mean(total_loss), np.mean(total_acc)), end='\r')
    backbone.train()
    header.train()
    return np.mean(total_loss), np.mean(total_acc)


def run_training(train_loader, valid_loader, backbone, header, epoch_num, lr, device, model_dir):
    best_acc = 0.0
    patience = 5
    counter = 0
    
    def check_point(backbone, header, loss, acc, model_dir):
        # Checkpoint saving only when accuracy improves
        nonlocal best_acc
        if acc > best_acc:
            best_acc = acc
            torch.save({
                'backbone': backbone.state_dict(), 
                'header': header.state_dict()
            }, model_dir)
            print(f'New best model saved with accuracy: {best_acc:.3f}')

    def is_stop(loss, acc):
        # TODO: Implement early stopping
        nonlocal counter, best_acc, patience
        
        if acc < best_acc:
            counter += 1
            print(f'EarlyStopping counter: {counter} out of {patience}')
            if counter >= patience:
                return True
        else:
            counter = 0
            
        return False

    if backbone is None:
        trainable_paras = header.parameters()
    else:
        trainable_paras = list(backbone.parameters()) + list(header.parameters())

    optimizer = torch.optim.Adam(trainable_paras, lr=lr, weight_decay=2e-5)

    # [新增] 定義 Scheduler
    # mode='max': 我們希望 Accuracy 越高越好
    # factor=0.5: 當卡住時，將學習率乘以 0.5 (砍半)
    # patience=2: 容忍 2 個 epoch 沒進步就降學習率 (設得比 early stop 小，先降速再停)
    # verbose=True: 降速時印出訊息通知
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=2, verbose=True
    )

    backbone.train()
    header.train()
    backbone = backbone.to(device)
    header = header.to(device)
    criterion = BinaryFocalLoss(alpha=0.5, gamma=2.0)
    
    for epoch in range(epoch_num):
        train(train_loader, backbone, header, optimizer, criterion, device, epoch)
        loss, acc = valid(valid_loader, backbone, header, criterion, device, epoch)
        
        print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f} '.format(epoch+1, loss, acc))
        
        # [新增] 更新 Scheduler
        # 告訴 scheduler 這次的 acc 是多少，讓它決定要不要降速
        scheduler.step(acc)

        check_point(backbone, header, loss, acc, model_dir) 
        if is_stop(loss, acc):
            print(f'Early stopping triggered after {epoch+1} epochs due to no improvement in accuracy for {patience} epochs.')
            break

"""## Testing"""

def run_ensemble_testing(test_loader, model_paths, lstm_config, header_config, preprocessor, device, output_path):
    print(f"Starting Ensemble Inference with {len(model_paths)} models...")
    
    # 1. 載入所有模型
    models = []
    for path in model_paths:
        print(f"Loading model from {path}...")
        backbone = LSTM_Backbone(preprocessor.embedding_matrix, **lstm_config).to(device)
        header = Header(**header_config).to(device)
        
        # 讀取權重
        checkpoint = torch.load(path)
        backbone.load_state_dict(checkpoint['backbone'])
        header.load_state_dict(checkpoint['header'])
        
        backbone.eval()
        header.eval()
        models.append((backbone, header))

    # 2. 開檔準備寫入 (newline='' 是為了防止 Windows 出現多餘空行)
    with open(output_path, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['id', 'label'])

        with torch.no_grad():
            for i, (idx_list, lengths, texts) in enumerate(test_loader):
                lengths, inputs = lengths.to(device), texts.to(device)
                
                batch_probs = [] 
                
                # 讓每個模型跑一次
                for backbone, header in models:
                    feat = backbone(inputs, lengths)
                    logits = header(feat, lengths)
                    probs = torch.sigmoid(logits) 
                    batch_probs.append(probs.cpu().numpy())
                
                # 取平均
                avg_probs = np.mean(batch_probs, axis=0)
                
                # 轉成 0/1
                hard_predicted = (avg_probs >= 0.5).astype(int)
                
                # [修正關鍵] 這裡要處理 Tensor 轉數值
                for id_val, p_val in zip(idx_list, hard_predicted):
                    # 判斷 id_val 是否為 Tensor，是的話取 .item()
                    if isinstance(id_val, torch.Tensor):
                        id_str = str(id_val.item())
                    else:
                        id_str = str(id_val)
                        
                    writer.writerow([id_str, str(p_val)])
                    
    print(f"Ensemble prediction saved to {output_path}")

def run_testing(test_loader, backbone, header, device, output_path):
    with open(output_path, 'w') as f:
        writer = csv.writer(f)
        writer.writerow(['id', 'label'])

        with torch.no_grad():
            for i, (idx_list, lengths, texts) in enumerate(test_loader):
                lengths, inputs = lengths.to(device), texts.to(device)
                if not backbone is None:
                    # [修正] 這裡要多傳入 lengths
                    inputs = backbone(inputs, lengths)
                    
                soft_predicted = header(inputs, lengths)
                
                # [修正] 同上
                hard_predicted = (soft_predicted >= 0).int()
                
                for i, p in zip(idx_list, hard_predicted):
                    writer.writerow([str(i.item()), str(p.item())])

# [新增] 1. 用於包裝偽標籤資料的輕量 Dataset
# 因為 get_pseudo_labels 抓出來的已經是轉換好的 Tensor，所以不需要再過 Preprocessor
class PseudoDataset(torch.utils.data.Dataset):
    def __init__(self, ids, texts, labels):
        self.data = []
        for i in range(len(ids)):
            self.data.append({'id': ids[i], 'text': texts[i], 'label': labels[i]})
    def __getitem__(self, idx):
        return self.data[idx]['id'], self.data[idx]['text'], self.data[idx]['label']
    def __len__(self): return len(self.data)
    # 直接複製你原本 Dataset 的 collate_fn 邏輯
    def collate_fn(self, data):
        id_list = [d[0] for d in data]
        texts = [d[1] for d in data]
        lengths = torch.LongTensor([len(t) for t in texts])
        texts_padded = pad_sequence(texts, batch_first=True).contiguous()
        labels = torch.FloatTensor([d[2] for d in data])
        return id_list, lengths, texts_padded, labels

# [新增] 2. 產生偽標籤的函式
def get_pseudo_labels(dataset, backbone, header, device, threshold=0.85):
    # 使用 shuffle=False 確保順序對應
    loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False, 
                                         collate_fn=dataset.collate_fn, num_workers=2)
    backbone.eval()
    header.eval()
    pseudo_samples = []
    
    print(f"Generating pseudo labels (Threshold: {threshold})...")
    with torch.no_grad():
        for i, (idx_list, lengths, texts) in enumerate(loader):
            lengths, inputs = lengths.to(device), texts.to(device)
            if backbone is not None:
                inputs = backbone(inputs, lengths)
            logits = header(inputs, lengths)
            probs = torch.sigmoid(logits).cpu().tolist()
            
            # 遍歷這個 Batch 的結果
            for j, prob in enumerate(probs):
                # 計算該樣本在整個 Dataset 中的絕對 index
                sample_idx = i * 128 + j
                if sample_idx >= len(dataset): break # 防止最後一個 batch越界

                if prob > threshold:
                    # 標記為 1 (正面)
                    # dataset.data[sample_idx]['text'] 已經是 tensor 了
                    raw_text_tensor = dataset.data[sample_idx]['text'] 
                    sample_id = dataset.data[sample_idx]['id']
                    pseudo_samples.append((sample_id, raw_text_tensor, 1.0))
                        
                elif prob < (1 - threshold):
                    # 標記為 0 (負面)
                    raw_text_tensor = dataset.data[sample_idx]['text']
                    sample_id = dataset.data[sample_idx]['id']
                    pseudo_samples.append((sample_id, raw_text_tensor, 0.0))
                        
    print(f"Generated {len(pseudo_samples)} pseudo-labeled samples.")
    return pseudo_samples

"""## Main"""
# 設定亂數種子的函式
def set_seed(seed):
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    np.random.seed(seed)

if __name__ == '__main__':
    # 1. 資料準備 (這部分只做一次)
    print("Loading data...")
    # 注意：這裡用原本的 load_train_nolabel 即可，因為不做 pseudo label，不需要 id
    train_idx, train_label_text, label = load_train_label('dataset/train_label.csv')
    
    # Word2Vec/FastText 訓練 (只做一次，因為它是無監督的，跟 Seed 關係較小)
    print("Initializing Word2Vec...")
    w2v_iterator = SentenceIterator(train_label_text, 'dataset/train_nolabel.csv')
    preprocessor = Preprocessor(w2v_iterator, w2v_config)

    # 準備 Test Set (只做一次)
    test_idx, test_text = load_test('dataset/test.csv')
    # [重要] 測試集 filter_short 必須為 False
    test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor, filter_short=False)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=test_dataset.collate_fn)

    # --- Ensemble 設定 ---
    SEEDS = [2023, 2024, 2025]  # 我們要跑這 3 個 Seed
    saved_models = []           # 用來存模型路徑

    # --- 開始訓練迴圈 ---
    for seed in SEEDS:
        print(f"\n{'='*15} Training with SEED {seed} {'='*15}")
        set_seed(seed) # 設定種子
        
        # 重新切分 Train/Valid (因為 Seed 變了，切分也會變，這能增加多樣性)
        curr_train_idx, curr_valid_idx, curr_train_text, curr_valid_text, curr_train_label, curr_valid_label = train_test_split(
            train_idx, train_label_text, label, test_size=0.1, random_state=seed
        )
        
        # 建立 Dataset
        train_dataset = TwitterDataset(curr_train_idx, curr_train_text, curr_train_label, preprocessor, filter_short=True)
        valid_dataset = TwitterDataset(curr_valid_idx, curr_valid_text, curr_valid_label, preprocessor, filter_short=False)
        
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn)
        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=valid_dataset.collate_fn)
        
        # 初始化模型
        backbone = LSTM_Backbone(preprocessor.embedding_matrix, **lstm_config).to(device)
        header = Header(**header_config).to(device)
        
        # 設定模型儲存名稱 (例如: model_2023.pth)
        current_model_name = f'model_{seed}.pth'
        
        # 開始訓練
        run_training(train_loader, valid_loader, backbone, header, EPOCH_NUM, lr, device, current_model_name)
        
        # 記錄路徑
        saved_models.append(current_model_name)
        
        # 釋放記憶體
        del backbone, header, train_loader, valid_loader
        torch.cuda.empty_cache()

    # --- 執行 Ensemble 預測 ---
    print("\nTraining Finished. Running Ensemble Inference...")
    run_ensemble_testing(
        test_loader=test_loader,
        model_paths=saved_models, # 傳入 ['model_2023.pth', 'model_2024.pth', ...]
        lstm_config=lstm_config,
        header_config=header_config,
        preprocessor=preprocessor,
        device=device,
        output_path='pred_ensemble.csv'
    )
    print("Done! Check 'pred_ensemble.csv'")