# -*- coding: utf-8 -*-
"""2025ML_HW4_sample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfYMQZdomFow5WZzQ-McS85xLBO6JFuW

# ML HW4 Sample Code
TODO:
 - Design your LSTM model
 - Use unlabelled data (train_nolabel.csv) for Word2Vec training
    - Combine labeled + unlabeled data to train better embeddings
 - Train with labelled data (train_label.csv)
    - Optional: Data augmentation
    - Optional: Custom loss function

## Download data
"""



"""## Import packages"""

import torch
import os
import csv
import random
import re
import math
import numpy as np
import pandas as pd
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from gensim.models import Word2Vec
from gensim.models.phrases import Phrases, Phraser
from gensim.models import FastText  # 引入 FastText
from sklearn.model_selection import train_test_split

"""## Set the Configurations"""

# !pip install -U gdown -q
# !gdown --folder https://drive.google.com/drive/folders/1786AXJRAtqFvWMBeh-bLm4MtU21IQpBg
# !pip install gensim
# Training Config
DEVICE_NUM = 2
BATCH_SIZE = 128
EPOCH_NUM = 50
MAX_POSITIONS_LEN = 500
SEED = 2025
MODEL_DIR = 'model.pth'
lr = 5e-4

# Set Seed
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
random.seed(SEED)
np.random.seed(SEED)

# torch.cuda.set_device(0)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# RNN Config
w2v_config = {'path': 'model_emoji_not_URL_repeat_quote_4', 'dim': 256}

# Transformer Config
# 注意：hidden_dim 必須能被 nhead (4) 整除
transformer_config = {
    'hidden_dim': 256,      # 建議加大一點，Transformer 喜歡高維度
    'num_layers': 2,        # 2~3 層通常就夠了，太深容易 train 不動
    'nhead': 4,             # Multi-head attention 的頭數
    'fix_embedding': False
}

header_config = {
    'dropout': 0.3,         # Transformer 比較不依賴 dropout 防止過擬合，可調低
    'hidden_dim': 256
}
# 確保維度對應正確 (這行原本的 assert 很好，留著)
# assert header_config['hidden_dim'] == lstm_config['hidden_dim'] or header_config['hidden_dim'] == lstm_config['hidden_dim'] * 2

"""## Utils for Datasets and Dataloaders"""

# 把 Regex 編譯放在函式外面，解決你之前遇到的卡頓問題
REGEX_URL = re.compile(r'http\S+')
REGEX_USER = re.compile(r'@\w+')
REGEX_HASHTAG = re.compile(r'#')
REGEX_REPEAT = re.compile(r'(.)\1{2,}')  # 抓出重複3次以上的字
REGEX_PUNCTUATION = re.compile(r'([^\w\s])') # 抓出所有非文字、非空白的符號
REGEX_SPACES = re.compile(r'\s+')

def parsing_text(text):
    if text is None or pd.isna(text):
        return ""
    
    text = str(text).lower() 

    # --- [新增] 先保護表情符號 ---
    # 定義字典映射
    emojis = {
        ':)': ' _smile_ ', ':-)': ' _smile_ ', ';)': ' _wink_ ', 
        ':(': ' _sad_ ',   ':-(': ' _sad_ ',  ':/': ' _worry_ ',
        '<3': ' _love_ '
    }
    for emo, token in emojis.items():
        text = text.replace(emo, token)
    # # ---------------------------
    
    # 1. 移除網址與 User ID (這些對情緒沒幫助)
    text = REGEX_URL.sub('', text)
    text = REGEX_USER.sub('', text)
    text = REGEX_HASHTAG.sub('', text) # 移除 # 符號，保留後面的文字
    
    # 2. 處理縮寫 (Contractions)
    # 手動加上空格，讓 .split(' ') 能把它們切開
    # 這對 Word2Vec 很重要，讓它學到 n't 就是 not
    text = text.replace("n't", " n't")
    text = text.replace("'s", " 's")
    text = text.replace("'m", " 'm")
    text = text.replace("'re", " 're")
    text = text.replace("'ve", " 've")
    text = text.replace("'ll", " 'll")
    text = text.replace("'d", " 'd")

    # 3. 處理重複字元 (Normalization)
    # 例如: "sooooo" -> "soo", "coool" -> "cool"
    # 邏輯：保留前兩個重複字元，後面的刪掉
    text = REGEX_REPEAT.sub(r'\1\1', text)

    # 4. [關鍵] 保留標點符號與表情
    # 原本是直接刪除，現在改成 "在前後加空白"
    # "good!" -> "good ! "
    # ":)" -> " : ) "
    # 這樣 split 後，"!" 和 ":" 就會變成獨立的 Token，模型就能學到它們的情緒權重
    text = REGEX_PUNCTUATION.sub(r' \1 ', text)
    
    # 5. 整理多餘空白
    text = REGEX_SPACES.sub(' ', text).strip()
    
    return text

def load_train_label(path='datatrain_label.csv'):
    tra_lb_pd = pd.read_csv(path)
    idx = tra_lb_pd['id'].tolist()
    text = [parsing_text(s).split() for s in tra_lb_pd['text'].tolist()]
    label = tra_lb_pd['label'].tolist()
    return idx, text, label

def load_train_nolabel(path='train_nolabel.csv'):
    tra_nlb_pd = pd.read_csv(path)
    text = [parsing_text(s).split() for s in tra_nlb_pd['text'].tolist()]
    return text

def load_test(path='test.csv'):
    test_pd = pd.read_csv(path)
    idx = test_pd['id'].tolist()
    text = [parsing_text(s).split() for s in test_pd['text'].tolist()]
    return idx, text

"""## Datasets and Dataloaders"""


class Preprocessor:
    def __init__(self, sentence_iterator, w2v_config):
        self.word2idx = {}
        self.idx2word = []
        self.embedding_matrix = []
        
        # [新增 1] 訓練 Bigram 模型 (自動偵測常見詞組)
        print("Training Bigram detector...")
        # min_count=5: 至少出現5次才算片語
        # threshold=10: 數值越高越嚴格 (越高代表兩個字黏得越緊)
        phrases = Phrases(sentence_iterator, min_count=5, threshold=10)
        self.bigram_transformer = Phraser(phrases)
        
        # [新增 2] 轉換原本的句子 (把 "not", "good" 變成 "not_good")
        # 我們寫一個生成器來轉換，避免吃記憶體
        class BigramIterator:
            def __init__(self, iterator, transformer):
                self.iterator = iterator
                self.transformer = transformer
            def __iter__(self):
                for sentence in self.iterator:
                    # 這行會把 ["new", "york"] 變成 ["new_york"]
                    yield self.transformer[sentence]

        bigram_sentences = BigramIterator(sentence_iterator, self.bigram_transformer)
        
        # 用轉換後的句子訓練 Word2Vec
        self.build_word2vec(bigram_sentences, **w2v_config)

    def build_word2vec(self, x, path, dim):
        if os.path.isfile(path):
            print("loading FastText model ...")
            # 注意這裡載入方式也不同
            w2v_model = FastText.load(path)
        else:
            print("training FastText model ...")
            # min_n, max_n 是字根長度範圍，Twitter 短字多，3-6 是好選擇
            w2v_model = FastText(vector_size=dim, window=5, min_count=3, 
                                workers=12, epochs=10, seed=SEED,
                                min_n=3, max_n=6) 
            w2v_model.build_vocab(x)
            w2v_model.train(x, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)
            
            print("saving FastText model ...")
            w2v_model.save(path)

        # 後面邏輯一樣，FastText 兼容 Word2Vec 的 wv 介面
        self.embedding_dim = w2v_model.vector_size
        self.embedding_dim = w2v_model.vector_size
        for i, word in enumerate(w2v_model.wv.key_to_index):
            #e.g. self.word2index['he'] = 1
            #e.g. self.index2word[1] = 'he'
            #e.g. self.vectors[1] = 'he' vector

            self.word2idx[word] = len(self.word2idx)
            self.idx2word.append(word)
            self.embedding_matrix.append(w2v_model.wv[word])

        self.embedding_matrix = torch.tensor(self.embedding_matrix)
        self.add_embedding('<PAD>')
        self.add_embedding('<UNK>')
        print("total words: {}".format(len(self.embedding_matrix)))

    def add_embedding(self, word):
        vector = torch.empty(1, self.embedding_dim)
        torch.nn.init.uniform_(vector)
        self.word2idx[word] = len(self.word2idx)
        self.idx2word.append(word)
        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)

    def sentence2idx(self, sentence):
        # 進來前先把句子轉成 Bigram
        sentence = self.bigram_transformer[sentence] 
        
        sentence_idx = []
        for word in sentence:
            if word in self.word2idx.keys():
                sentence_idx.append(self.word2idx[word])
            else:
                sentence_idx.append(self.word2idx["<UNK>"])
        return torch.LongTensor(sentence_idx)

class SentenceIterator:
    def __init__(self, labeled_data, unlabeled_path):
        self.labeled_data = labeled_data  # 這是已經讀到記憶體裡的 labeled list
        self.unlabeled_path = unlabeled_path

    def __iter__(self):
        # 1. 先回傳有標註的資料 (已經在 RAM 裡了)
        for sentence in self.labeled_data:
            yield sentence
            
        # 2. 接著從硬碟串流讀取無標註資料 (省記憶體關鍵！)
        import csv
        # 使用標準 csv library 逐行讀取，不要用 pandas 一次讀
        with open(self.unlabeled_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader) # 跳過 header ('id', 'text')
            
            for row in reader:
                # 確保格式正確，row[1] 是 text
                if len(row) > 1:
                    # 這裡記得要用跟 labeled data 一樣的處理方式
                    text = parsing_text(row[1])
                    yield text.split(' ')

class TwitterDataset(torch.utils.data.Dataset):
    def __init__(self, id_list, sentences, labels, preprocessor):
        self.data = []
        self.sentences = sentences
        self.labels = labels
        self.id_list = id_list
        self.preprocessor = preprocessor
        # 設定最小長度門檻
        MIN_LENGTH = 4
        
        for i in range(len(sentences)):
            input_ids = preprocessor.sentence2idx(sentences[i])
            
            # [關鍵] 只保留長度 > 3 的句子
            # 太短的句子通常沒辦法判斷情緒，只會變成雜訊
            if len(input_ids) >= MIN_LENGTH:
                label = labels[i] if labels is not None else None
                self.data.append({
                    'id': id_list[i],
                    'text': input_ids,
                    'label': label
                })
        print(f"Filtered {len(sentences) - len(self.data)} short/empty samples.")

    def __getitem__(self, idx):
        if self.labels is None: return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx])
        return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx]), self.labels[idx]

    def __len__(self):
        return len(self.sentences)

    def collate_fn(self, data):
        id_list = torch.LongTensor([d[0] for d in data])
        lengths = torch.LongTensor([len(d[1]) for d in data])
        texts = pad_sequence(
            [d[1] for d in data], batch_first=True).contiguous()

        if self.labels is None:
            return id_list, lengths, texts

        labels = torch.FloatTensor([d[2] for d in data])
        return id_list, lengths, texts, labels

"""## RNN Backbone"""



class PositionalEncoding(torch.nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=500):
        super(PositionalEncoding, self).__init__()
        self.dropout = torch.nn.Dropout(p=dropout)

        # 預先計算位置編碼矩陣
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0) # (1, max_len, d_model)
        
        # register_buffer 會將 pe 存入 state_dict，但不會被視為可訓練參數
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: (Batch, Seq_Len, Dim)
        # 取出對應長度的位置編碼並加上去
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class Transformer_Backbone(torch.nn.Module):
    def __init__(self, embedding, hidden_dim, num_layers, nhead=4, fix_embedding=False):
        super(Transformer_Backbone, self).__init__()
        
        self.embedding_dim = embedding.size(1)
        self.hidden_dim = hidden_dim
        
        # 1. Embedding Layer
        self.embedding = torch.nn.Embedding(embedding.size(0), self.embedding_dim)
        self.embedding.weight = torch.nn.Parameter(embedding)
        self.embedding.weight.requires_grad = False if fix_embedding else True
        
        # 線性層將 Embedding 轉為 hidden_dim (若維度不同)
        self.fc_in = torch.nn.Linear(self.embedding_dim, hidden_dim) if self.embedding_dim != hidden_dim else torch.nn.Identity()
        
        # 2. Positional Encoding
        self.pos_encoder = PositionalEncoding(hidden_dim, dropout=0.1)
        
        # 3. Transformer Encoder
        # batch_first=True 讓輸入格式維持 (Batch, Seq, Feature)
        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, 
                                                         dim_feedforward=hidden_dim*4, 
                                                         dropout=0.1, batch_first=True)
        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, inputs, lengths):
        # inputs: (Batch, Seq_Len)
        
        # 1. 產生 Padding Mask (True 代表要忽略的位置)
        # Transformer 需要知道哪些位置是 Pad，不能做 Attention
        # 建立一個 (Batch, Seq_Len) 的 Boolean Mask
        batch_size, max_len = inputs.shape
        # 產生 [0, 1, 2, ... max_len-1]
        range_tensor = torch.arange(max_len, device=inputs.device).unsqueeze(0).expand(batch_size, max_len)
        # 如果 index >= length，則為 True (被 Mask 掉)
        padding_mask = range_tensor >= lengths.unsqueeze(1)
        
        # 2. Embedding
        embeds = self.embedding(inputs) # (Batch, Seq_Len, Emb_Dim)
        
        # 3. 調整維度並加上 Positional Encoding
        # Transformer 的論文建議將 embedding 乘上 sqrt(d_model)
        embeds = self.fc_in(embeds) * math.sqrt(self.hidden_dim)
        embeds = self.pos_encoder(embeds)
        
        # 4. 通過 Transformer
        # src_key_padding_mask: (Batch, Seq_Len)
        out = self.transformer_encoder(embeds, src_key_padding_mask=padding_mask)
        
        return out

class Header(torch.nn.Module):
    def __init__(self, dropout, hidden_dim, num_heads=4):
        super(Header, self).__init__()
        
        self.num_heads = num_heads
        self.hidden_dim = hidden_dim
        
        # 確保 hidden_dim 能被 num_heads 整除
        assert hidden_dim % num_heads == 0
        self.head_dim = hidden_dim // num_heads
        
        # [模組 1] 多頭 Attention
        # 我們用一個大的 Linear 層算出所有頭的分數，然後再切開
        self.attention_linear = torch.nn.Linear(hidden_dim, num_heads)
        
        # [模組 2] 最終分類器
        # 因為有 num_heads 個頭，每個頭產出 head_dim 的向量
        # 拼接後維度依然是 hidden_dim
        self.classifier = torch.nn.Sequential(
            torch.nn.LayerNorm(hidden_dim),  # 改用 LayerNorm
            torch.nn.Dropout(dropout),
            
            torch.nn.Linear(hidden_dim, hidden_dim // 2),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout),
            
            torch.nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, inputs, lengths):
        # inputs: (Batch, Seq_Len, Hidden)
        batch_size = inputs.size(0)
        seq_len = inputs.size(1)
        
        # 1. 計算每個頭的 Attention Score
        # scores: (Batch, Seq_Len, Num_Heads)
        scores = self.attention_linear(inputs)
        
        # 2. 製作 Mask
        # (Batch, Seq_Len, 1) -> 廣播到 (Batch, Seq_Len, Num_Heads)
        mask = torch.arange(seq_len, device=inputs.device)[None, :] < lengths[:, None]
        mask = mask.unsqueeze(-1).expand(-1, -1, self.num_heads)
        
        # 3. Apply Mask & Softmax
        scores = scores.masked_fill(~mask, -1e9)
        # 在 Seq_Len 維度做 Softmax
        weights = F.softmax(scores, dim=1) # (Batch, Seq_Len, Num_Heads)
        
        # 4. 加權總和 (Weighted Sum) - 這是最 tricky 的部分
        # 我們希望每個頭只負責 input 的一部分特徵 (head_dim)
        
        # 先把 input 切成多個頭: (Batch, Seq_Len, Num_Heads, Head_Dim)
        inputs_reshaped = inputs.view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # 把 weights 擴展維度以便相乘: (Batch, Seq_Len, Num_Heads, 1)
        weights_expanded = weights.unsqueeze(-1)
        
        # 相乘並加總: (Batch, Seq_Len, Num_Heads, Head_Dim) -> Sum over Seq_Len
        # Result: (Batch, Num_Heads, Head_Dim)
        context_vectors = torch.sum(inputs_reshaped * weights_expanded, dim=1)
        
        # 5. 拼接 (Concatenate)
        # 把所有頭接回來: (Batch, Num_Heads * Head_Dim) -> (Batch, Hidden)
        context_vectors = context_vectors.view(batch_size, -1)
        
        # 6. 分類
        out = self.classifier(context_vectors).squeeze()
        
        return out
    
"""## Training & Validation"""

# 定義 KL Divergence Loss
def compute_kl_loss(p, q, pad_mask=None):
    p_loss = F.kl_div(F.log_softmax(p, dim=-1), F.softmax(q, dim=-1), reduction='none')
    q_loss = F.kl_div(F.log_softmax(q, dim=-1), F.softmax(p, dim=-1), reduction='none')
    
    # pad_mask is optional, generally average over batch is fine
    if pad_mask is not None:
        p_loss.masked_fill_(pad_mask, 0.)
        q_loss.masked_fill_(pad_mask, 0.)

    p_loss = p_loss.sum()
    q_loss = q_loss.sum()

    loss = (p_loss + q_loss) / 2
    return loss

class BinaryFocalLoss(torch.nn.Module):
    """
    Focal Loss for Binary Classification
    Formula: Loss = - alpha * (1 - p_t)^gamma * log(p_t)
    """
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super(BinaryFocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, logits, targets):
        # logits: (Batch_Size, ), 未經過 Sigmoid
        # targets: (Batch_Size, ), 0 或 1
        
        # 1. 計算原本的 Binary Cross Entropy
        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
        
        # 2. 取得預測機率 p_t
        # 如果 target=1, p_t = p; 如果 target=0, p_t = 1-p
        p = torch.sigmoid(logits)
        p_t = p * targets + (1 - p) * (1 - targets)
        
        # 3. 計算調節因子 (Modulating Factor): (1 - p_t)^gamma
        # 當 p_t 接近 1 (預測準確)，這個因子會趨近 0 -> 降低權重
        # 當 p_t 接近 0 (預測錯誤/難樣本)，這個因子會很大 -> 保持權重
        loss = self.alpha * (1 - p_t)**self.gamma * bce_loss
        
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss
        
# 修改 train 函數
def train(train_loader, backbone, header, optimizer, criterion, device, epoch):
    alpha = 2 # R-Drop 的權重 (通常設 1~5)

    for i, (idx_list, lengths, texts, labels) in enumerate(train_loader):
        lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)
        optimizer.zero_grad()
        
        # --- R-Drop 核心邏輯 ---
        # 1. 把 inputs 複製一份，變成兩倍大 (Batch*2, Seq_Len)
        inputs_doubled = torch.cat([inputs, inputs], dim=0)
        lengths_doubled = torch.cat([lengths, lengths], dim=0)
        
        # 2. 過模型 (因為有 Dropout，所以前半段和後半段出來的結果會不同)
        if backbone is not None:
            feats = backbone(inputs_doubled, lengths_doubled)
        logits = header(feats, lengths_doubled) # 注意：Header 最後不能有 Sigmoid
        
        # 3. 切開結果
        batch_size = inputs.size(0)
        logits1 = logits[:batch_size]
        logits2 = logits[batch_size:]
        soft_predicted = (logits1 + logits2) / 2
        # 4. 計算原本的 Loss (兩次都要算)
        # 注意：Loss Function 必須是 BCEWithLogitsLoss
        loss_nll = (criterion(logits1, labels) + criterion(logits2, labels)) / 2
        
        # 5. 計算 KL Loss (強迫兩次預測一致)
        # 對於二元分類，可以簡化為 MSE 或直接用 logits 算
        # 這裡為了簡單，我們用 MSE 近似 KL (對 Logits)
        loss_kl = compute_kl_loss(logits1, logits2) 
        
        # 總 Loss
        loss = loss_nll + alpha * loss_kl
        
        loss.backward()

        torch.nn.utils.clip_grad_norm_(backbone.parameters(), max_norm=1.0) # 建議設 1.0 或 5.0
        
        optimizer.step()
        

        with torch.no_grad():
            # [修正] 如果 Header 移除了 Sigmoid (搭配 BCEWithLogitsLoss)，這裡要改用 >= 0
            # 如果你 Header 還留著 Sigmoid，這裡維持 >= 0.5
            hard_predicted = (soft_predicted >= 0).int() 
            correct = sum(hard_predicted == labels).item()
            batch_size = len(labels)
            print('[ Epoch {}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(epoch+1,i+1, len(train_loader), loss.item(), correct * 100 / batch_size), end='\r')


def valid(valid_loader, backbone, header, criterion, device, epoch):
    backbone.eval()
    header.eval()
    with torch.no_grad():
        total_loss = []
        total_acc = []

        for i, (idx_list, lengths, texts, labels) in enumerate(valid_loader):
            lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)

            if not backbone is None:
                # [修正] 這裡要多傳入 lengths
                inputs = backbone(inputs, lengths)
                
            soft_predicted = header(inputs, lengths)
            loss = criterion(soft_predicted, labels)
            total_loss.append(loss.item())

            # [修正] 同上，配合 Loss Function 決定是 0 或 0.5
            hard_predicted = (soft_predicted >= 0).int()
            correct = sum(hard_predicted == labels).item()
            acc = correct * 100 / len(labels)
            total_acc.append(acc)

            print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f}'.format(epoch+1, np.mean(total_loss), np.mean(total_acc)), end='\r')
    backbone.train()
    header.train()
    return np.mean(total_loss), np.mean(total_acc)


def run_training(train_loader, valid_loader, backbone, header, epoch_num, lr, device, model_dir):
    best_acc = 0.0
    patience = 5
    counter = 0
    
    def check_point(backbone, header, loss, acc, model_dir):
        # Checkpoint saving only when accuracy improves
        nonlocal best_acc
        if acc > best_acc:
            best_acc = acc
            torch.save({'backbone': backbone, 'header': header}, model_dir)
            print(f'New best model saved with accuracy: {best_acc:.3f}')

    def is_stop(loss, acc):
        # TODO: Implement early stopping
        nonlocal counter, best_acc, patience
        
        if acc < best_acc:
            counter += 1
            print(f'EarlyStopping counter: {counter} out of {patience}')
            if counter >= patience:
                return True
        else:
            counter = 0
            
        return False

    if backbone is None:
        trainable_paras = header.parameters()
    else:
        trainable_paras = list(backbone.parameters()) + list(header.parameters())

    optimizer = torch.optim.Adam(trainable_paras, lr=lr, weight_decay=2e-5)

    # [新增] 定義 Scheduler
    # mode='max': 我們希望 Accuracy 越高越好
    # factor=0.5: 當卡住時，將學習率乘以 0.5 (砍半)
    # patience=2: 容忍 2 個 epoch 沒進步就降學習率 (設得比 early stop 小，先降速再停)
    # verbose=True: 降速時印出訊息通知
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=2, verbose=True
    )

    backbone.train()
    header.train()
    backbone = backbone.to(device)
    header = header.to(device)
    criterion = BinaryFocalLoss(alpha=0.5, gamma=2.0)
    
    for epoch in range(epoch_num):
        train(train_loader, backbone, header, optimizer, criterion, device, epoch)
        loss, acc = valid(valid_loader, backbone, header, criterion, device, epoch)
        
        print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f} '.format(epoch+1, loss, acc))
        
        # [新增] 更新 Scheduler
        # 告訴 scheduler 這次的 acc 是多少，讓它決定要不要降速
        scheduler.step(acc)

        check_point(backbone, header, loss, acc, model_dir) 
        if is_stop(loss, acc):
            print(f'Early stopping triggered after {epoch+1} epochs due to no improvement in accuracy for {patience} epochs.')
            break

"""## Testing"""

def run_testing(test_loader, backbone, header, device, output_path):
    with open(output_path, 'w') as f:
        writer = csv.writer(f)
        writer.writerow(['id', 'label'])

        with torch.no_grad():
            for i, (idx_list, lengths, texts) in enumerate(test_loader):
                lengths, inputs = lengths.to(device), texts.to(device)
                if not backbone is None:
                    # [修正] 這裡要多傳入 lengths
                    inputs = backbone(inputs, lengths)
                    
                soft_predicted = header(inputs, lengths)
                
                # [修正] 同上
                hard_predicted = (soft_predicted >= 0).int()
                
                for i, p in zip(idx_list, hard_predicted):
                    writer.writerow([str(i.item()), str(p.item())])

"""## Main"""
if __name__ == '__main__':
    # Split Dataset
    train_idx, train_label_text, label = load_train_label('dataset/train_label.csv')
    train_nolabel_text = load_train_nolabel('dataset/train_nolabel.csv')

    # For sanity check
    print(train_nolabel_text[:10])

    # Use labeled data for Word2Vec embeddings (# TODO: Perform unsupervised Learning for w2v)
    # 讀取 labeled 和 unlabeled 資料

    # [修改這裡] 合併兩份資料集給 Word2Vec 訓練
    # 這樣字典會更完整，且向量語意更準確
    print("Initializing Sentence Iterator for Word2Vec...")
    # 傳入 labeled list 和 unlabeled 的檔案路徑
    w2v_iterator = SentenceIterator(train_label_text, 'dataset/train_nolabel.csv')

    # 4. 訓練 Word2Vec (Preprocessor 會把它傳給 Gensim)
    # Gensim 會自動呼叫 __iter__ 跑完所有資料
    preprocessor = Preprocessor(w2v_iterator, w2v_config)

    train_idx, valid_idx, train_label_text, valid_label_text, train_label, valid_label = train_test_split(train_idx, train_label_text, label, test_size=0.12)
    
    train_dataset, valid_dataset = TwitterDataset(train_idx, train_label_text, train_label, preprocessor), TwitterDataset(valid_idx, valid_label_text, valid_label, preprocessor)

    test_idx, test_text = load_test('dataset/test.csv')
    test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor)

    train_loader = torch.utils.data.DataLoader(dataset = train_dataset,
                                                batch_size = BATCH_SIZE,
                                                shuffle = True,
                                                collate_fn = train_dataset.collate_fn,
                                                num_workers = 0)
    valid_loader = torch.utils.data.DataLoader(dataset = valid_dataset,
                                                batch_size = BATCH_SIZE,
                                                shuffle = False,
                                                collate_fn = valid_dataset.collate_fn,
                                                num_workers = 0)
    test_loader = torch.utils.data.DataLoader(dataset = test_dataset,
                                                batch_size = BATCH_SIZE,
                                                shuffle = False,
                                                collate_fn = test_dataset.collate_fn,
                                                num_workers = 0)

    # Instantiate The Model
    backbone = Transformer_Backbone(preprocessor.embedding_matrix, **transformer_config)
    header = Header(**header_config)

    

    # Run Training
    run_training(train_loader, valid_loader, backbone, header, EPOCH_NUM, lr, device, MODEL_DIR)

    # Run Testing
    run_testing(test_loader, backbone, header, device, 'pred.csv')