# -*- coding: utf-8 -*-
"""2025ML_HW4_sample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfYMQZdomFow5WZzQ-McS85xLBO6JFuW

# ML HW4 Sample Code
TODO:
 - Design your LSTM model
 - Use unlabelled data (train_nolabel.csv) for Word2Vec training
    - Combine labeled + unlabeled data to train better embeddings
 - Train with labelled data (train_label.csv)
    - Optional: Data augmentation
    - Optional: Custom loss function

## Download data
"""



"""## Import packages"""

import torch
import os
import csv
import random
import re
import numpy as np
import pandas as pd
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from gensim.models import Word2Vec
from gensim.models.phrases import Phrases, Phraser
from sklearn.model_selection import train_test_split

"""## Set the Configurations"""

# !pip install -U gdown -q
# !gdown --folder https://drive.google.com/drive/folders/1786AXJRAtqFvWMBeh-bLm4MtU21IQpBg
# !pip install gensim
# Training Config
DEVICE_NUM = 2
BATCH_SIZE = 64
EPOCH_NUM = 50
MAX_POSITIONS_LEN = 500
SEED = 0
MODEL_DIR = 'model.pth'
lr = 0.001

# Set Seed
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
random.seed(SEED)
np.random.seed(SEED)

# torch.cuda.set_device(0)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# RNN Config
w2v_config = {'path': 'model_5', 'dim': 256}

lstm_config = {
    'hidden_dim': 128,
    'num_layers': 1,        # 有了 Pack 和 Attention，1 層通常就夠強且夠快
    'bidirectional': True,
    'fix_embedding': False  # 建議解凍，因為我們有 Unlabeled Data 強化過 Word2Vec
}

header_config = {
    'dropout': 0.2,         # 因為解凍了 Embedding，Dropout 可以高一點防止過擬合
    'hidden_dim': 256       # 256 * 2
}

# 確保維度對應正確 (這行原本的 assert 很好，留著)
assert header_config['hidden_dim'] == lstm_config['hidden_dim'] or header_config['hidden_dim'] == lstm_config['hidden_dim'] * 2

"""## Utils for Datasets and Dataloaders"""

# 把 Regex 編譯放在函式外面，解決你之前遇到的卡頓問題
REGEX_URL = re.compile(r'http\S+')
REGEX_USER = re.compile(r'@\w+')
REGEX_HASHTAG = re.compile(r'#')
REGEX_REPEAT = re.compile(r'(.)\1{2,}')  # 抓出重複3次以上的字
REGEX_PUNCTUATION = re.compile(r'([^\w\s])') # 抓出所有非文字、非空白的符號
REGEX_SPACES = re.compile(r'\s+')

def parsing_text(text):
    if text is None or pd.isna(text):
        return ""
    
    text = str(text).lower() 
    
    # 1. 移除網址與 User ID (這些對情緒沒幫助)
    text = REGEX_URL.sub('', text)
    text = REGEX_USER.sub('', text)
    text = REGEX_HASHTAG.sub('', text) # 移除 # 符號，保留後面的文字
    
    # 2. 處理縮寫 (Contractions)
    # 手動加上空格，讓 .split(' ') 能把它們切開
    # 這對 Word2Vec 很重要，讓它學到 n't 就是 not
    text = text.replace("n't", " n't")
    text = text.replace("'s", " 's")
    text = text.replace("'m", " 'm")
    text = text.replace("'re", " 're")
    text = text.replace("'ve", " 've")
    text = text.replace("'ll", " 'll")
    text = text.replace("'d", " 'd")

    # 3. 處理重複字元 (Normalization)
    # 例如: "sooooo" -> "soo", "coool" -> "cool"
    # 邏輯：保留前兩個重複字元，後面的刪掉
    text = REGEX_REPEAT.sub(r'\1\1', text)

    # 4. [關鍵] 保留標點符號與表情
    # 原本是直接刪除，現在改成 "在前後加空白"
    # "good!" -> "good ! "
    # ":)" -> " : ) "
    # 這樣 split 後，"!" 和 ":" 就會變成獨立的 Token，模型就能學到它們的情緒權重
    text = REGEX_PUNCTUATION.sub(r' \1 ', text)
    
    # 5. 整理多餘空白
    text = REGEX_SPACES.sub(' ', text).strip()
    
    return text

def load_train_label(path='datatrain_label.csv'):
    tra_lb_pd = pd.read_csv(path)
    idx = tra_lb_pd['id'].tolist()
    text = [parsing_text(s).split() for s in tra_lb_pd['text'].tolist()]
    label = tra_lb_pd['label'].tolist()
    return idx, text, label

def load_train_nolabel(path='train_nolabel.csv'):
    tra_nlb_pd = pd.read_csv(path)
    text = [parsing_text(s).split() for s in tra_nlb_pd['text'].tolist()]
    return text

def load_test(path='test.csv'):
    test_pd = pd.read_csv(path)
    idx = test_pd['id'].tolist()
    text = [parsing_text(s).split() for s in test_pd['text'].tolist()]
    return idx, text

"""## Datasets and Dataloaders"""


class Preprocessor:
    def __init__(self, sentence_iterator, w2v_config):
        self.word2idx = {}
        self.idx2word = []
        self.embedding_matrix = []
        
        # [新增 1] 訓練 Bigram 模型 (自動偵測常見詞組)
        print("Training Bigram detector...")
        # min_count=5: 至少出現5次才算片語
        # threshold=10: 數值越高越嚴格 (越高代表兩個字黏得越緊)
        phrases = Phrases(sentence_iterator, min_count=5, threshold=10)
        self.bigram_transformer = Phraser(phrases)
        
        # [新增 2] 轉換原本的句子 (把 "not", "good" 變成 "not_good")
        # 我們寫一個生成器來轉換，避免吃記憶體
        class BigramIterator:
            def __init__(self, iterator, transformer):
                self.iterator = iterator
                self.transformer = transformer
            def __iter__(self):
                for sentence in self.iterator:
                    # 這行會把 ["new", "york"] 變成 ["new_york"]
                    yield self.transformer[sentence]

        bigram_sentences = BigramIterator(sentence_iterator, self.bigram_transformer)
        
        # 用轉換後的句子訓練 Word2Vec
        self.build_word2vec(bigram_sentences, **w2v_config)

    def build_word2vec(self, x, path, dim):
        if os.path.isfile(path):
            print("loading word2vec model ...")
            w2v_model = Word2Vec.load(path)
        else:
            print("training word2vec model ...")
            w2v_model = Word2Vec(x, vector_size=dim, window=5, min_count=3, workers=12, epochs=15, sg=1)
            print("saving word2vec model ...")
            w2v_model.save(path)

        self.embedding_dim = w2v_model.vector_size
        for i, word in enumerate(w2v_model.wv.key_to_index):
            #e.g. self.word2index['he'] = 1
            #e.g. self.index2word[1] = 'he'
            #e.g. self.vectors[1] = 'he' vector

            self.word2idx[word] = len(self.word2idx)
            self.idx2word.append(word)
            self.embedding_matrix.append(w2v_model.wv[word])

        self.embedding_matrix = torch.tensor(self.embedding_matrix)
        self.add_embedding('<PAD>')
        self.add_embedding('<UNK>')
        print("total words: {}".format(len(self.embedding_matrix)))

    def add_embedding(self, word):
        vector = torch.empty(1, self.embedding_dim)
        torch.nn.init.uniform_(vector)
        self.word2idx[word] = len(self.word2idx)
        self.idx2word.append(word)
        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)

    def sentence2idx(self, sentence):
        # 進來前先把句子轉成 Bigram
        sentence = self.bigram_transformer[sentence] 
        
        sentence_idx = []
        for word in sentence:
            if word in self.word2idx.keys():
                sentence_idx.append(self.word2idx[word])
            else:
                sentence_idx.append(self.word2idx["<UNK>"])
        return torch.LongTensor(sentence_idx)

class SentenceIterator:
    def __init__(self, labeled_data, unlabeled_path):
        self.labeled_data = labeled_data  # 這是已經讀到記憶體裡的 labeled list
        self.unlabeled_path = unlabeled_path

    def __iter__(self):
        # 1. 先回傳有標註的資料 (已經在 RAM 裡了)
        for sentence in self.labeled_data:
            yield sentence
            
        # 2. 接著從硬碟串流讀取無標註資料 (省記憶體關鍵！)
        import csv
        # 使用標準 csv library 逐行讀取，不要用 pandas 一次讀
        with open(self.unlabeled_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader) # 跳過 header ('id', 'text')
            
            for row in reader:
                # 確保格式正確，row[1] 是 text
                if len(row) > 1:
                    # 這裡記得要用跟 labeled data 一樣的處理方式
                    text = parsing_text(row[1])
                    yield text.split(' ')

class TwitterDataset(torch.utils.data.Dataset):
    def __init__(self, id_list, sentences, labels, preprocessor):
        self.data = []
        self.sentences = sentences
        self.labels = labels
        self.id_list = id_list
        self.preprocessor = preprocessor
        # 設定最小長度門檻
        MIN_LENGTH = 3 
        
        for i in range(len(sentences)):
            input_ids = preprocessor.sentence2idx(sentences[i])
            
            # [關鍵] 只保留長度 > 3 的句子
            # 太短的句子通常沒辦法判斷情緒，只會變成雜訊
            if len(input_ids) >= MIN_LENGTH:
                label = labels[i] if labels is not None else None
                self.data.append({
                    'id': id_list[i],
                    'text': input_ids,
                    'label': label
                })
        print(f"Filtered {len(sentences) - len(self.data)} short/empty samples.")

    def __getitem__(self, idx):
        if self.labels is None: return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx])
        return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx]), self.labels[idx]

    def __len__(self):
        return len(self.sentences)

    def collate_fn(self, data):
        id_list = torch.LongTensor([d[0] for d in data])
        lengths = torch.LongTensor([len(d[1]) for d in data])
        texts = pad_sequence(
            [d[1] for d in data], batch_first=True).contiguous()

        if self.labels is None:
            return id_list, lengths, texts

        labels = torch.FloatTensor([d[2] for d in data])
        return id_list, lengths, texts, labels

"""## RNN Backbone"""

class LSTM_Backbone(torch.nn.Module):
    def __init__(self, embedding, hidden_dim, num_layers, bidirectional, fix_embedding=False):
        super(LSTM_Backbone, self).__init__()
        self.embedding = torch.nn.Embedding(embedding.size(0), embedding.size(1))
        self.embedding.weight = torch.nn.Parameter(embedding)
        self.embedding.weight.requires_grad = False if fix_embedding else True
        
        # [新增] Spatial Dropout (雖然是用 Dropout2d 實作，但對 Text 是一樣的效果)
        # 建議設 0.3 ~ 0.4
        self.embedding_dropout = torch.nn.Dropout2d(0.3) 

        self.lstm = torch.nn.LSTM(embedding.size(1), hidden_dim, num_layers=num_layers, \
                                  bidirectional=bidirectional, batch_first=True)

    def forward(self, inputs, lengths):
        # inputs: (Batch, Seq_Len)
        embeds = self.embedding(inputs) # (Batch, Seq_Len, Dim)
        
        # [新增] 實作 Spatial Dropout
        # Dropout2d 期望輸入是 (Batch, Channel, Height, Width)
        # 我們把 Dim 當作 Channel
        if self.training:
            # 1. 轉置: (Batch, Dim, Seq_Len)
            embeds = embeds.permute(0, 2, 1)  
            # 2. 增加一個維度變成 (Batch, Dim, Seq_Len, 1) 騙過 Dropout2d
            embeds = embeds.unsqueeze(3)
            # 3. 丟棄整條 Feature
            embeds = self.embedding_dropout(embeds)
            # 4. 還原形狀: (Batch, Seq_Len, Dim)
            embeds = embeds.squeeze(3).permute(0, 2, 1)
        
        # 接下來是原本的 Pack logic
        packed_input = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths.cpu(), batch_first=True, enforce_sorted=False)
        packed_output, (hidden, cell) = self.lstm(packed_input)
        out, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)
        
        return out

class Header(torch.nn.Module):
    def __init__(self, dropout, hidden_dim, num_heads=4):
        super(Header, self).__init__()
        
        self.num_heads = num_heads
        self.hidden_dim = hidden_dim
        
        # 確保 hidden_dim 能被 num_heads 整除
        assert hidden_dim % num_heads == 0
        self.head_dim = hidden_dim // num_heads
        
        # [模組 1] 多頭 Attention
        # 我們用一個大的 Linear 層算出所有頭的分數，然後再切開
        self.attention_linear = torch.nn.Linear(hidden_dim, num_heads)
        
        # [模組 2] 最終分類器
        # 因為有 num_heads 個頭，每個頭產出 head_dim 的向量
        # 拼接後維度依然是 hidden_dim
        self.classifier = torch.nn.Sequential(
            torch.nn.LayerNorm(hidden_dim),  # 改用 LayerNorm
            torch.nn.Dropout(dropout),
            
            torch.nn.Linear(hidden_dim, hidden_dim // 2),
            torch.nn.ReLU(),
            torch.nn.Dropout(dropout),
            
            torch.nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, inputs, lengths):
        # inputs: (Batch, Seq_Len, Hidden)
        batch_size = inputs.size(0)
        seq_len = inputs.size(1)
        
        # 1. 計算每個頭的 Attention Score
        # scores: (Batch, Seq_Len, Num_Heads)
        scores = self.attention_linear(inputs)
        
        # 2. 製作 Mask
        # (Batch, Seq_Len, 1) -> 廣播到 (Batch, Seq_Len, Num_Heads)
        mask = torch.arange(seq_len, device=inputs.device)[None, :] < lengths[:, None]
        mask = mask.unsqueeze(-1).expand(-1, -1, self.num_heads)
        
        # 3. Apply Mask & Softmax
        scores = scores.masked_fill(~mask, -1e9)
        # 在 Seq_Len 維度做 Softmax
        weights = F.softmax(scores, dim=1) # (Batch, Seq_Len, Num_Heads)
        
        # 4. 加權總和 (Weighted Sum) - 這是最 tricky 的部分
        # 我們希望每個頭只負責 input 的一部分特徵 (head_dim)
        
        # 先把 input 切成多個頭: (Batch, Seq_Len, Num_Heads, Head_Dim)
        inputs_reshaped = inputs.view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # 把 weights 擴展維度以便相乘: (Batch, Seq_Len, Num_Heads, 1)
        weights_expanded = weights.unsqueeze(-1)
        
        # 相乘並加總: (Batch, Seq_Len, Num_Heads, Head_Dim) -> Sum over Seq_Len
        # Result: (Batch, Num_Heads, Head_Dim)
        context_vectors = torch.sum(inputs_reshaped * weights_expanded, dim=1)
        
        # 5. 拼接 (Concatenate)
        # 把所有頭接回來: (Batch, Num_Heads * Head_Dim) -> (Batch, Hidden)
        context_vectors = context_vectors.view(batch_size, -1)
        
        # 6. 分類
        out = self.classifier(context_vectors).squeeze()
        
        return out
    
"""## Training & Validation"""

# 定義 KL Divergence Loss
def compute_kl_loss(p, q, pad_mask=None):
    p_loss = F.kl_div(F.log_softmax(p, dim=-1), F.softmax(q, dim=-1), reduction='none')
    q_loss = F.kl_div(F.log_softmax(q, dim=-1), F.softmax(p, dim=-1), reduction='none')
    
    # pad_mask is optional, generally average over batch is fine
    if pad_mask is not None:
        p_loss.masked_fill_(pad_mask, 0.)
        q_loss.masked_fill_(pad_mask, 0.)

    p_loss = p_loss.sum()
    q_loss = q_loss.sum()

    loss = (p_loss + q_loss) / 2
    return loss

# 修改 train 函數
def train(train_loader, backbone, header, optimizer, criterion, device, epoch):
    alpha = 2 # R-Drop 的權重 (通常設 1~5)

    for i, (idx_list, lengths, texts, labels) in enumerate(train_loader):
        lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)
        optimizer.zero_grad()
        
        # --- R-Drop 核心邏輯 ---
        # 1. 把 inputs 複製一份，變成兩倍大 (Batch*2, Seq_Len)
        inputs_doubled = torch.cat([inputs, inputs], dim=0)
        lengths_doubled = torch.cat([lengths, lengths], dim=0)
        
        # 2. 過模型 (因為有 Dropout，所以前半段和後半段出來的結果會不同)
        if backbone is not None:
            feats = backbone(inputs_doubled, lengths_doubled)
        logits = header(feats, lengths_doubled) # 注意：Header 最後不能有 Sigmoid
        
        # 3. 切開結果
        batch_size = inputs.size(0)
        logits1 = logits[:batch_size]
        logits2 = logits[batch_size:]
        soft_predicted = (logits1 + logits2) / 2
        # 4. 計算原本的 Loss (兩次都要算)
        # 注意：Loss Function 必須是 BCEWithLogitsLoss
        loss_nll = (criterion(logits1, labels) + criterion(logits2, labels)) / 2
        
        # 5. 計算 KL Loss (強迫兩次預測一致)
        # 對於二元分類，可以簡化為 MSE 或直接用 logits 算
        # 這裡為了簡單，我們用 MSE 近似 KL (對 Logits)
        loss_kl = compute_kl_loss(logits1, logits2) 
        
        # 總 Loss
        loss = loss_nll + alpha * loss_kl
        
        loss.backward()

        torch.nn.utils.clip_grad_norm_(backbone.parameters(), max_norm=1.0) # 建議設 1.0 或 5.0
        
        optimizer.step()
        

        with torch.no_grad():
            # [修正] 如果 Header 移除了 Sigmoid (搭配 BCEWithLogitsLoss)，這裡要改用 >= 0
            # 如果你 Header 還留著 Sigmoid，這裡維持 >= 0.5
            hard_predicted = (soft_predicted >= 0).int() 
            correct = sum(hard_predicted == labels).item()
            batch_size = len(labels)
            print('[ Epoch {}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(epoch+1,i+1, len(train_loader), loss.item(), correct * 100 / batch_size), end='\r')


def valid(valid_loader, backbone, header, criterion, device, epoch):
    backbone.eval()
    header.eval()
    with torch.no_grad():
        total_loss = []
        total_acc = []

        for i, (idx_list, lengths, texts, labels) in enumerate(valid_loader):
            lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)

            if not backbone is None:
                # [修正] 這裡要多傳入 lengths
                inputs = backbone(inputs, lengths)
                
            soft_predicted = header(inputs, lengths)
            loss = criterion(soft_predicted, labels)
            total_loss.append(loss.item())

            # [修正] 同上，配合 Loss Function 決定是 0 或 0.5
            hard_predicted = (soft_predicted >= 0).int()
            correct = sum(hard_predicted == labels).item()
            acc = correct * 100 / len(labels)
            total_acc.append(acc)

            print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f}'.format(epoch+1, np.mean(total_loss), np.mean(total_acc)), end='\r')
    backbone.train()
    header.train()
    return np.mean(total_loss), np.mean(total_acc)


def run_training(train_loader, valid_loader, backbone, header, epoch_num, lr, device, model_dir):
    best_acc = 0.0
    patience = 5
    counter = 0
    
    def check_point(backbone, header, loss, acc, model_dir):
        # Checkpoint saving only when accuracy improves
        nonlocal best_acc
        if acc > best_acc:
            best_acc = acc
            torch.save({'backbone': backbone, 'header': header}, model_dir)
            print(f'New best model saved with accuracy: {best_acc:.3f}')

    def is_stop(loss, acc):
        # TODO: Implement early stopping
        nonlocal counter, best_acc, patience
        
        if acc < best_acc:
            counter += 1
            print(f'EarlyStopping counter: {counter} out of {patience}')
            if counter >= patience:
                return True
        else:
            counter = 0
            
        return False

    if backbone is None:
        trainable_paras = header.parameters()
    else:
        trainable_paras = list(backbone.parameters()) + list(header.parameters())

    optimizer = torch.optim.Adam(trainable_paras, lr=lr, weight_decay=2e-5)

    # [新增] 定義 Scheduler
    # mode='max': 我們希望 Accuracy 越高越好
    # factor=0.5: 當卡住時，將學習率乘以 0.5 (砍半)
    # patience=2: 容忍 2 個 epoch 沒進步就降學習率 (設得比 early stop 小，先降速再停)
    # verbose=True: 降速時印出訊息通知
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=2, verbose=True
    )

    backbone.train()
    header.train()
    backbone = backbone.to(device)
    header = header.to(device)
    criterion = torch.nn.BCEWithLogitsLoss()
    
    for epoch in range(epoch_num):
        train(train_loader, backbone, header, optimizer, criterion, device, epoch)
        loss, acc = valid(valid_loader, backbone, header, criterion, device, epoch)
        
        print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f} '.format(epoch+1, loss, acc))
        
        # [新增] 更新 Scheduler
        # 告訴 scheduler 這次的 acc 是多少，讓它決定要不要降速
        scheduler.step(acc)

        check_point(backbone, header, loss, acc, model_dir) 
        if is_stop(loss, acc):
            print(f'Early stopping triggered after {epoch+1} epochs due to no improvement in accuracy for {patience} epochs.')
            break

"""## Testing"""

def run_testing(test_loader, backbone, header, device, output_path):
    with open(output_path, 'w') as f:
        writer = csv.writer(f)
        writer.writerow(['id', 'label'])

        with torch.no_grad():
            for i, (idx_list, lengths, texts) in enumerate(test_loader):
                lengths, inputs = lengths.to(device), texts.to(device)
                if not backbone is None:
                    # [修正] 這裡要多傳入 lengths
                    inputs = backbone(inputs, lengths)
                    
                soft_predicted = header(inputs, lengths)
                
                # [修正] 同上
                hard_predicted = (soft_predicted >= 0).int()
                
                for i, p in zip(idx_list, hard_predicted):
                    writer.writerow([str(i.item()), str(p.item())])

"""## Main"""
if __name__ == '__main__':
    # Split Dataset
    train_idx, train_label_text, label = load_train_label('dataset/train_label.csv')
    train_nolabel_text = load_train_nolabel('dataset/train_nolabel.csv')

    # For sanity check
    print(train_nolabel_text[:10])

    # Use labeled data for Word2Vec embeddings (# TODO: Perform unsupervised Learning for w2v)
    # 讀取 labeled 和 unlabeled 資料

    # [修改這裡] 合併兩份資料集給 Word2Vec 訓練
    # 這樣字典會更完整，且向量語意更準確
    print("Initializing Sentence Iterator for Word2Vec...")
    # 傳入 labeled list 和 unlabeled 的檔案路徑
    w2v_iterator = SentenceIterator(train_label_text, 'dataset/train_nolabel.csv')

    # 4. 訓練 Word2Vec (Preprocessor 會把它傳給 Gensim)
    # Gensim 會自動呼叫 __iter__ 跑完所有資料
    preprocessor = Preprocessor(w2v_iterator, w2v_config)

    train_idx, valid_idx, train_label_text, valid_label_text, train_label, valid_label = train_test_split(train_idx, train_label_text, label, test_size=0.12)
    
    train_dataset, valid_dataset = TwitterDataset(train_idx, train_label_text, train_label, preprocessor), TwitterDataset(valid_idx, valid_label_text, valid_label, preprocessor)

    test_idx, test_text = load_test('dataset/test.csv')
    test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor)

    train_loader = torch.utils.data.DataLoader(dataset = train_dataset,
                                                batch_size = BATCH_SIZE,
                                                shuffle = True,
                                                collate_fn = train_dataset.collate_fn,
                                                num_workers = 0)
    valid_loader = torch.utils.data.DataLoader(dataset = valid_dataset,
                                                batch_size = BATCH_SIZE,
                                                shuffle = False,
                                                collate_fn = valid_dataset.collate_fn,
                                                num_workers = 0)
    test_loader = torch.utils.data.DataLoader(dataset = test_dataset,
                                                batch_size = BATCH_SIZE,
                                                shuffle = False,
                                                collate_fn = test_dataset.collate_fn,
                                                num_workers = 0)

    # Instantiate The Model
    backbone = LSTM_Backbone(preprocessor.embedding_matrix, **lstm_config)
    header = Header(**header_config)

    

    # Run Training
    run_training(train_loader, valid_loader, backbone, header, EPOCH_NUM, lr, device, MODEL_DIR)

    # Run Testing
    run_testing(test_loader, backbone, header, device, 'pred.csv')