\documentclass[12pt,a4paper]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{interval}
\usepackage{amssymb}
\usepackage{xeCJK}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=1cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{ML Written Homework 1}
\author{Student: R14921A13鄭皓中}

\begin{document}
\maketitle

\section{Linear Algebra Recap}

\begin{enumerate}
    \item [(a)] 
        (i) $\Rightarrow$ (ii) \textbf{A} is positive semi-definite implies \textbf{A} is symmetric. Since \textbf{A} is symmetric, \textbf{A} is diagonalizable.
        Suppose $\lambda_{1}, \lambda_{2}, ..., \lambda_{n}$ are the eigenvalues of \textbf{A}, there are eigenvectors $v_{1}, v_{2}, ..., v_{n}$ such that $\textbf{A}v_{k} = \lambda_{k}v_{k}$.

        By the definition of positive semi-definite, we have
        \[
            \left\langle \textbf{A}v_{k}, v_{k}\right\rangle = \left\langle \lambda_{k}v_{k}, v_{k} \right\rangle = \lambda_{k}\left\lVert v_k \right\rVert^2  \geq 0
        \]
        Hence, all eigenvalues $\lambda_{k} \geq 0$.

        (ii) $\Rightarrow$ (iii) \textbf{A} is symmetric and all of eigenvalues $\lambda_{k}$ are non negative. Since \textbf{A} is symmetric, \textbf{A} is diagonalizable.
        That is, there are $P, D$ such that $\textbf{A} = P^{T}DP$, where D = $diag(\lambda_{1}, \lambda_{2}, ..., \lambda_{n})$.

        Since $\lambda_{k} \geq 0$, we can find $e_{k} = \sqrt{\lambda_{k}}$ and $E = diag(e_{1}, e_{2}, ..., e_{n})$ so that $D = E^2$.
        Now we have
        \[
            \mathbf{A} = P^{T}DP = P^{T}EEP = (P^{T}E)(EP) = (EP)^{T}(EP)
        \]
        Hence, $\mathbf{A} = \mathbf{B}^{T}\mathbf{B}$ where $\mathbf{B} = EP$

        (iii) $\Rightarrow$ (i) $\mathbf{A} = \mathbf{B}^{T}\mathbf{B}$, then we have
        \[
            u^{T}\mathbf{A}u = u^{T}\mathbf{B}^{T} \mathbf{B}u = (\mathbf{B}u)^{T}(\mathbf{B}u) = \left\lVert \mathbf{B}u \right\rVert ^2 \geq 0
        \]
        for all $u \in V$. Hence, \textbf{A} is positive semi-definite. 
    \item[(b)]
        Note that $\left\langle \mathbf{A}^{T}\mathbf{A} x, x \right\rangle = \left\langle \mathbf{A}x, \mathbf{A}x\right\rangle = \left\lVert \mathbf{A}x \right\rVert^2 \geq 0$ for all $x\in V$.
        By (a), $\mathbf{A}^{T}\mathbf{A}$ is positive semi-definite. Hence the eigenvalues of $\mathbf{A}^{T}\mathbf{A}$ are nonnegative.
    \item[(c)]
        For $x\in Null(\mathbf{A})$, we have $\mathbf{A}^{T}\mathbf{A}x = 0$, so that $x\in Null(\mathbf{A}^{T}\mathbf{A})$.
        
        For $x\in Null(\mathbf{A}^{T}\mathbf{A})$, 
        \[
            \left\langle \mathbf{A}^{T}\mathbf{A}x, x \right\rangle =  \left\langle \mathbf{A}x, \mathbf{A}x\right\rangle =\left\lVert \mathbf{A}x \right\rVert^{2} = 0
        \]
        which implies $\mathbf{A}x = 0$. Hence $x\in Null(\mathbf{A})$.
        By the above, we can find that $Null(\mathbf{A}) = Null(\mathbf{A}^{T}\mathbf{A})$.
    \item[(d)]
        $\left\langle \mathbf{A}v_{i}, \mathbf{A}v_{j} \right\rangle = \left\langle \mathbf{A}^{T}\mathbf{A} v_{i}, v_{j} \right\rangle = \left\langle \lambda_{i}v_{i}, v_{j} \right\rangle = 0$ for any $i \neq j$. Hence $\{\mathbf{A}v_{1}, ..., \mathbf{A}v_{r}\}$ is an orthogonal set.
    \item[(e)]
        Suppose $V$ is a $n \times n$ matrix whose $i$-th column is $v_{i}$, then $V$ is an orthogonal matrix where $\mathbf{A}^{T}\mathbf{A} = VDV^{T}$. Let $U$ be a $m \times m$ matrix with its $i$-th column $u_{i}$, where $u_{i} = \frac{\mathbf{A}v_{i}}{\sigma_{i}}$, and $\Sigma$ be a $m \times n$ matrix with its diagonal entries $\sigma_{i}$.

        Then we have the relation:
        \[
            U\Sigma = \mathbf{A}V \Rightarrow \mathbf{A} = U\Sigma V^{T}
        \]
        By (d), $\{\mathbf{A}v_{1}, ..., \mathbf{A}v_{r}\}$ is an orthogonal set, we have $\{\mathbf{u}_{1}, ..., \mathbf{u}_{r}\}$ is also an orthogonal set.
        Hence $U$ is also an orthogonal matrix.
\end{enumerate}

\section{Definition of Derivative as Linear Operator}    

\begin{enumerate}
    \item[(a)]
        Suppose $\mathbf{x} = (x_{1}, x_{2}, ..., x_{n})^{T}, \mathbf{a} = (a_{1}, a_{2}, ..., a_{n})^{T}$. $\left\lVert \mathbf{x} - \mathbf{a} \right\rVert _{2} = \sqrt{\sum_{i = 1}^{n} (x_{i} - a_{i})^{2}}  $.
        \[
            \frac{\partial \sqrt{\sum_{i = 1}^{n} (x_{i} - a_{i})^{2}}}{\partial x_{k}} = \frac{1}{2}\cdot \frac{2(x_{k} - a_{k})}{\sqrt{\sum_{i = 1}^{n} (x_{i} - a_{i})^{2}}} = \frac{x_{k} - a_{k}}{\left\lVert \mathbf{x} - \mathbf{a} \right\rVert _{2}}
        \]
        Then we have 
        \[
            \frac{\partial \left\lVert \mathbf{x} - \mathbf{a} \right\rVert _{2}}{\partial \mathbf{x}} = (\frac{x_{1} - a_{1}}{\left\lVert \mathbf{x} - \mathbf{a} \right\rVert _{2}}, ..., \frac{x_{n} - a_{n}}{\left\lVert \mathbf{x} - \mathbf{a} \right\rVert _{2}})^{T} = \frac{\mathbf{x} - \mathbf{a}}{\left\lVert \mathbf{x} - \mathbf{a} \right\rVert _{2}}
        \]
    \item[(b)]
        Suppose $\mathbf{a} = (a_{1}, ..., a_{m})^{T}, \mathbf{b} = (b_{1}, ..., b_{n})^{T}, \mathbf{X} = (x_{ij})$. 
        \[
            \mathbf{a}^{T}\mathbf{X}\mathbf{T} = \sum_{j=1}^{n}\sum_{i=1}^{m}a_{i}x_{ij}b_{j}
        \]
        \[
            \frac{\partial \mathbf{a}^{T}\mathbf{X}\mathbf{T}}{\partial x_{ij}} = \frac{\sum_{j=1}^{n}\sum_{i=1}^{m}a_{i}x_{ij}b_{j}}{\partial x_{ij}} = a_{i}b_{j}
        \]
        Then we have
        \[
            \frac{\partial \mathbf{a}^{T}\mathbf{X}\mathbf{T}}{\partial \mathbf{X}} = \mathbf{a}\mathbf{b}^{T}
        \]
    \item[(c)]
        Suppose $\mathbf{x} = (x_{1}, ..., x_{n})^{T}, \mathbf{A} = (A_{ij})$.
        \[
            \mathbf{x}^{T}\mathbf{A}\mathbf{x} = \sum_{j=1}^{n}\sum_{i=1}^{n}x_{i}A_{ij}x_{j}
        \] 
        \[
            \frac{\partial \mathbf{x}^{T}\mathbf{A}\mathbf{x}}{\partial x_{k}} = \frac{\partial \sum_{j=1}^{n}\sum_{i=1}^{n}x_{i}A_{ij}x_{j}}{\partial x_k} = \sum_{i=1}^{n}x_{i}A_{ik} + \sum_{j=1}^{n}A_{kj}x_{j} = (\mathbf{A}^{T}\mathbf{x})_{k} + (\mathbf{A} \mathbf{x})_{k}
        \]
        Hence, we have
        \[
            \frac{\partial \mathbf{x}^{T}\mathbf{A}\mathbf{x}}{\partial \mathbf{x}} = (\mathbf{A} + \mathbf{A}^{T}) \mathbf{x}
        \]
    
\end{enumerate}

\section{Matrix Calculus}    

\begin{enumerate}
    \item[(a)] 
        We have $\det(\mathbf{A}) = \sum_{j=1}^{n}\sum_{i=1}^{n} A_{ij} * C_{ij}$, where $C_{ij} = (-1)^{i+j}M_{ij} = adj(\mathbf{A})_{ji}$.
        Thus we have
        \[
            \frac{\partial \det(\mathbf{A})}{\partial A_{ij}} = C_{ij} = adj(\mathbf{A})_{ji}
        \]
        Hence, 
        \[
            \frac{\partial \det(\mathbf{A})}{\partial \mathbf{A}} = adj(\mathbf{A})^{T} = \det(\mathbf{A})(\mathbf{A}^{-1})^{T}
        \]

    \item[(b)]
        By chain rule:
        \[
            \frac{\partial \log(\det(\mathbf{A}))}{\partial \mathbf{A}} = \frac{\partial \log(\det(\mathbf{A}))}{\partial \det(\mathbf{A})} \frac{\partial \det(\mathbf{A})}{\partial \mathbf{A}} = \frac{1}{\det(\mathbf{A})}\det(\mathbf{A})(\mathbf{A}^{-1})^{T} = (\mathbf{A}^{-1})^{T}
        \]

\end{enumerate}

\section{Closed-Form Linear Regression Solution}

\begin{enumerate}
    \item[(a)] 
        Suppose $\mathbf{K}=diag(\kappa_{1}, ..., \kappa_{m})$
        \[
            \sum_{i}^{m}\kappa_{i}(y_{i} - \mathbf{X}_{i}\mathbf{\theta})^{2} + \lambda \sum_{j}^{m} \omega_{j}^{2} = (y-\mathbf{X}\theta)^{T}\mathbf{K}(y-\mathbf{X}\theta) + \lambda \theta^{T}\theta
        \]
        The optimal solution happens when the gradient = 0
        \[
            \frac{\partial (\sum_{i}^{m}\kappa_{i}(y_{i} - \mathbf{X}_{i}\mathbf{\theta})^{2} + \lambda \sum_{j}^{m} \omega_{j}^{2})}{\partial \theta} = -2\mathbf{X}^{T}\mathbf{K}(y-\mathbf{X}\theta) + 2\lambda \mathbf{I}\theta = 0
        \]
        This gives us 
        \[
            \theta^{*} = (\mathbf{X}^{T}\mathbf{K}\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^{T}\mathbf{K}y
        \]
    \item[(b)]
\end{enumerate}

\section{Noise and Regularization}

\begin{enumerate}
    \item[(a)]
        \begin{align}
            \frac{1}{2N}\sum_{i=1}^{N}(f_{\mathbf{w},b}(\mathbf{x}_{i}+\eta_{i})-y_{i})^{2} 
                &= \frac{1}{2N}\sum_{i=1}^{N}(\mathbf{w}^{T}\mathbf{x}_i + b - y_{i} + \mathbf{w}^{T}\eta_{i})^2 \\
                &= \frac{1}{2N}\sum_{i=1}^{N}(f_{\mathbf{w}, b}(\mathbf{x}_i) - y_{i} + \mathbf{w}^{T}\eta_{i})^2 \\
                &= \frac{1}{2N}\sum_{i=1}^{N}(f_{\mathbf{w}, b}(\mathbf{x}_i) - y_{i})^2 + (\mathbf{w}^{T}\eta_{i})^2 + 2(f_{\mathbf{w}, b}(\mathbf{x}_i) - y_{i})(\mathbf{w}^{T}\eta_{i})
        \end{align}
        Since expect value $\mathbb{E}$ is linear, we can seperate the formula into three part.
        \[
            \mathbb{E}((f_{\mathbf{w},b}(\mathbf{x}_{i})-y_{i})^2) = (f_{\mathbf{w},b}(\mathbf{x}_{i})-y_{i})^2
        \]
        \[
            \mathbb{E}((\mathbf{w}^{T}\eta_{i})^2) = \sigma^{2}\left\lVert \mathbf{w}\right\rVert ^{2}
        \]
        \[
            2(f_{\mathbf{w}, b}(\mathbf{x}_i) - y_{i})(\mathbf{w}^{T}\eta_{i}) = 2(f_{\mathbf{w}, b}(\mathbf{x}_i) - y_{i}) \sum_{j=1}^{k} w_{j}\eta_{i,j}
        \]
        \[
            \mathbb{E}(2(f_{\mathbf{w}, b}(\mathbf{x}_i) - y_{i}) \sum_{j=1}^{k} w_{j}\eta_{i,j}) = 2(f_{\mathbf{w}, b}(\mathbf{x}_i) - y_{i}) \sum_{j=1}^{k} w_{j}\mathbb{E}(\eta_{i,j})=0
        \]
        \[
            \tilde{L} (\mathbf{w}, b) = \mathbb{E}(\frac{1}{2N}\sum_{i=1}^{N}(f_{\mathbf{w},b}(\mathbf{x}_{i}+\eta_{i})-y_{i})^{2})
        \]
        \begin{align}
            \tilde{L}_{ss} (\mathbf{w}, b) 
                &= \mathbb{E}(\frac{1}{2N}\sum_{i=1}^{N}(f_{\mathbf{w},b}(\mathbf{x}_{i}+\eta_{i})-y_{i})^{2}) \\
                &= \frac{1}{2N}\sum_{i=1}^{N}((f_{\mathbf{w},b}(\mathbf{x}_{i})-y_{i})^2 + \sigma^{2}\left\lVert \mathbf{w}\right\rVert ^{2})\\
                &= \frac{1}{2N}\sum_{i=1}^{N}(f_{\mathbf{w},b}(\mathbf{x}_{i})-y_{i})^2 + \frac{\sigma^{2}}{2}\left\lVert \mathbf{w}\right\rVert ^{2}
        \end{align}
\end{enumerate}

\end{document}
